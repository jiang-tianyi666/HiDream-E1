# HiDream-E1 NPU ç‰ˆæœ¬ä½¿ç”¨æŒ‡å—

æœ¬æ–‡æ¡£è¯´æ˜å¦‚ä½•åœ¨åä¸ºæ˜‡è…¾ NPU ä¸Šè¿è¡Œ HiDream-E1 å›¾åƒç¼–è¾‘æ¨¡å‹ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒå‡†å¤‡

1. **å®‰è£… CANN (å¿…éœ€)**
   ```bash
   # å‚è€ƒåä¸ºå®˜æ–¹æ–‡æ¡£å®‰è£… CANN toolkit å’Œ kernels
   # https://www.hiascend.com/document
   ```

2. **å®‰è£… PyTorch å’Œ torch_npu**
   ```bash
   # æ ¹æ®ä½ çš„ CANN ç‰ˆæœ¬å®‰è£…å¯¹åº”çš„ torch
   pip install torch==2.1.0  # ç‰ˆæœ¬å·éœ€è¦ä¸ CANN åŒ¹é…
   pip install torch_npu      # åä¸º NPU æ’ä»¶

   # éªŒè¯å®‰è£…
   python -c "import torch; import torch_npu; print(torch.npu.is_available())"
   ```

3. **å®‰è£…é¡¹ç›®ä¾èµ–**
   ```bash
   pip install -r requirements_npu.txt
   ```

4. **ç™»å½• HuggingFace**
   ```bash
   huggingface-cli login
   # éœ€è¦åŒæ„ Llama-3.1-8B-Instruct çš„ä½¿ç”¨åè®®
   ```

### è¿è¡Œæ¨ç†

#### æ–¹æ³• 1ï¼šä½¿ç”¨ NPU ç‰ˆæœ¬è„šæœ¬ï¼ˆæ¨èï¼‰

```bash
# HiDream-E1.1 ç‰ˆæœ¬ï¼ˆæ¨èï¼‰
python inference_e1_1_npu.py

# HiDream-E1-Full ç‰ˆæœ¬ï¼ˆå¸¦ LoRAï¼‰
python inference_npu.py
```

#### æ–¹æ³• 2ï¼šä½¿ç”¨ Gradio äº¤äº’ç•Œé¢

```bash
python gradio_demo_npu.py
```

ç„¶ååœ¨æµè§ˆå™¨æ‰“å¼€ `http://localhost:7860`

### æŒ‡å®šè®¾å¤‡

é»˜è®¤æƒ…å†µä¸‹ä¼šè‡ªåŠ¨æ£€æµ‹è®¾å¤‡ï¼ˆNPU > CUDA > CPUï¼‰ï¼Œä¹Ÿå¯ä»¥æ‰‹åŠ¨æŒ‡å®šï¼š

```bash
# å¼ºåˆ¶ä½¿ç”¨ NPU
export PREFERRED_DEVICE=npu
python inference_e1_1_npu.py

# å¼ºåˆ¶ä½¿ç”¨ CUDAï¼ˆå¦‚æœéœ€è¦å¯¹æ¯”ï¼‰
export PREFERRED_DEVICE=cuda
python inference_e1_1_npu.py

# å¼ºåˆ¶ä½¿ç”¨ CPU
export PREFERRED_DEVICE=cpu
python inference_e1_1_npu.py
```

## ğŸ“ ä»£ç ä¿®æ”¹è¯´æ˜

### æ–°å¢æ–‡ä»¶

1. **device_utils.py** - è®¾å¤‡ç®¡ç†å·¥å…·
   - è‡ªåŠ¨æ£€æµ‹ NPU/CUDA/CPU
   - è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„æ•°æ®ç±»å‹ï¼ˆbfloat16/float16/float32ï¼‰
   - ç»Ÿä¸€çš„å†…å­˜ç®¡ç†æ¥å£

2. **inference_e1_1_npu.py** - NPU ç‰ˆæœ¬æ¨ç†è„šæœ¬ï¼ˆE1.1ï¼‰
3. **inference_npu.py** - NPU ç‰ˆæœ¬æ¨ç†è„šæœ¬ï¼ˆE1-Fullï¼‰
4. **gradio_demo_npu.py** - NPU ç‰ˆæœ¬äº¤äº’ç•Œé¢

### å…³é”®ä¿®æ”¹ç‚¹

1. **è®¾å¤‡æŒ‡å®š**
   ```python
   # åŸä»£ç 
   pipe.to("cuda", torch.bfloat16)

   # NPU ç‰ˆæœ¬
   from device_utils import DeviceManager
   dm = DeviceManager()
   pipe.to(dm.device, dm.dtype)
   ```

2. **éšæœºç”Ÿæˆå™¨**
   ```python
   # åŸä»£ç 
   generator = torch.Generator("cuda").manual_seed(seed)

   # NPU ç‰ˆæœ¬
   generator = dm.create_generator(seed)
   ```

3. **å†…å­˜ç›‘æ§**
   ```python
   # åŸä»£ç 
   torch.cuda.memory_summary(device='cuda')

   # NPU ç‰ˆæœ¬
   dm.memory_stats()  # è‡ªåŠ¨é€‚é… NPU/CUDA
   ```

4. **Flash Attention ç¦ç”¨**
   ```python
   # NPU ä¸æ”¯æŒ CUDA Flash Attention
   os.environ["DIFFUSERS_ATTENTION_TYPE"] = "vanilla"
   ```

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. æ€§èƒ½é¢„æœŸ

- **é¦–æ¬¡è¿è¡Œ**ï¼šå¯èƒ½è¾ƒæ…¢ï¼ˆæ¨¡å‹ä¸‹è½½ã€ç¼–è¯‘ï¼‰
- **æ²¡æœ‰ Flash Attention**ï¼šæ¨ç†é€Ÿåº¦å¯èƒ½æ¯” CUDA ç‰ˆæœ¬æ…¢ 30-50%
- **å†…å­˜å ç”¨**ï¼šç±»ä¼¼ CUDA ç‰ˆæœ¬ï¼Œçº¦éœ€ 16GB+ æ˜¾å­˜

### 2. ç²¾åº¦æ”¯æŒ

è®¾å¤‡ç®¡ç†å™¨ä¼šè‡ªåŠ¨æ£€æµ‹å¹¶é€‰æ‹©æœ€ä½³ç²¾åº¦ï¼š
- ä¼˜å…ˆä½¿ç”¨ `bfloat16`ï¼ˆå¦‚æœæ”¯æŒï¼‰
- é™çº§åˆ° `float16`ï¼ˆå¦‚æœ bfloat16 ä¸æ”¯æŒï¼‰
- æœ€åé™çº§åˆ° `float32`ï¼ˆå¦‚æœå‰ä¸¤è€…éƒ½ä¸æ”¯æŒï¼‰

### 3. å·²çŸ¥é—®é¢˜

#### é—®é¢˜ 1ï¼šç®—å­ä¸æ”¯æŒ
**ç°è±¡**ï¼š`RuntimeError: operator XXX is not implemented for NPU`

**è§£å†³**ï¼š
```bash
# å¯ç”¨å›é€€æ¨¡å¼ï¼ˆä¸æ”¯æŒçš„ç®—å­è‡ªåŠ¨å›é€€åˆ° CPUï¼‰
export NPU_FALLBACK_MODE=1
python inference_e1_1_npu.py
```

#### é—®é¢˜ 2ï¼šå†…å­˜æº¢å‡º
**è§£å†³**ï¼š
```python
# åœ¨è„šæœ¬ä¸­å¯ç”¨ CPU offload
pipe.enable_model_cpu_offload()
```

#### é—®é¢˜ 3ï¼šLlama æ¨¡å‹åŠ è½½å¤±è´¥
**è§£å†³**ï¼š
```bash
# ç¡®ä¿å·²ç™»å½• HuggingFace å¹¶åŒæ„åè®®
huggingface-cli login
```

## ğŸ§ª æµ‹è¯•å’ŒéªŒè¯

### åŸºç¡€æµ‹è¯•

```bash
# 1. æµ‹è¯•è®¾å¤‡æ£€æµ‹
python device_utils.py

# 2. æµ‹è¯•æ¨ç†ï¼ˆå°æ­¥æ•°ï¼‰
python -c "
from device_utils import DeviceManager
dm = DeviceManager()
print(f'Device: {dm.device}')
print(f'Data type: {dm.dtype}')
"
```

### æ€§èƒ½å¯¹æ¯”

```bash
# åœ¨ NPU ä¸Šè¿è¡Œ
time python inference_e1_1_npu.py

# åœ¨ CUDA ä¸Šè¿è¡Œï¼ˆå¦‚æœæœ‰ï¼‰
export PREFERRED_DEVICE=cuda
time python inference_e1_1_npu.py
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. ä½¿ç”¨æ··åˆç²¾åº¦

å¦‚æœé‡åˆ°ç²¾åº¦é—®é¢˜ï¼Œå¯ä»¥è®© VAE ä½¿ç”¨æ›´é«˜ç²¾åº¦ï¼š

```python
# åœ¨ init_models() åæ·»åŠ 
pipe.vae = pipe.vae.to(torch.float32)
```

### 2. å‡å°‘æ¨ç†æ­¥æ•°

```python
# åœ¨ edit_image() ä¸­è°ƒæ•´
steps=20  # ä»é»˜è®¤çš„ 28 å‡å°‘åˆ° 20
```

### 3. é™ä½åˆ†è¾¨ç‡

```python
# ä¿®æ”¹ resize_image å‡½æ•°
def resize_image(pil_image, image_size=768):  # ä» 1024 é™åˆ° 768
    ...
```

### 4. æ‰¹å¤„ç†ä¼˜åŒ–

å¦‚æœå¤„ç†å¤šå¼ å›¾ç‰‡ï¼š

```python
# ä½¿ç”¨æ‰¹å¤„ç†
prompts = ["instruction1", "instruction2", "instruction3"]
images = [img1, img2, img3]
results = pipe(prompt=prompts, image=images, ...)
```

## ğŸ” è°ƒè¯•æŠ€å·§

### 1. å¯ç”¨è¯¦ç»†æ—¥å¿—

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

### 2. æ£€æŸ¥æ¨¡å‹ç²¾åº¦

```python
print(f"Transformer dtype: {pipe.transformer.dtype}")
print(f"VAE dtype: {pipe.vae.dtype}")
print(f"Text encoder dtype: {pipe.text_encoder_4.dtype}")
```

### 3. é€ç»„ä»¶æµ‹è¯•

```python
# æµ‹è¯• VAE
dummy_img = torch.randn(1, 3, 512, 512).to(dm.device, dm.dtype)
latent = pipe.vae.encode(dummy_img).latent_dist.sample()
print("âœ“ VAE works")

# æµ‹è¯• Text Encoder
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.1-8B-Instruct")
inputs = tokenizer("test", return_tensors="pt").to(dm.device)
outputs = pipe.text_encoder_4(**inputs)
print("âœ“ Text encoder works")
```

## ğŸ“ è·å–å¸®åŠ©

- **åä¸ºæ˜‡è…¾ç¤¾åŒº**ï¼šhttps://www.hiascend.com/forum
- **CANN æ–‡æ¡£**ï¼šhttps://www.hiascend.com/document
- **torch_npu GitHub**ï¼šhttps://gitee.com/ascend/pytorch
- **é¡¹ç›® Issues**ï¼šæäº¤åˆ°åŸé¡¹ç›®æˆ–è”ç³»å¼€å‘è€…

## ğŸ“„ æ–‡ä»¶å¯¹ç…§è¡¨

| åŸå§‹æ–‡ä»¶ | NPU ç‰ˆæœ¬ | è¯´æ˜ |
|---------|---------|------|
| inference_e1_1.py | inference_e1_1_npu.py | E1.1 æ¨ç†è„šæœ¬ |
| inference.py | inference_npu.py | E1-Full æ¨ç†è„šæœ¬ |
| gradio_demo_1_1.py | gradio_demo_npu.py | äº¤äº’ç•Œé¢ |
| requirements.txt | requirements_npu.txt | ä¾èµ–åˆ—è¡¨ |
| - | device_utils.py | è®¾å¤‡ç®¡ç†å·¥å…·ï¼ˆæ–°å¢ï¼‰ |
| - | README_NPU.md | NPU ä½¿ç”¨æŒ‡å—ï¼ˆæ–°å¢ï¼‰ |

## âœ… è¿ç§»æ£€æŸ¥æ¸…å•

- [ ] å·²å®‰è£… CANN toolkit å’Œ kernels
- [ ] å·²å®‰è£… torch å’Œ torch_npu
- [ ] `torch.npu.is_available()` è¿”å› `True`
- [ ] å·²å®‰è£…é¡¹ç›®ä¾èµ– `pip install -r requirements_npu.txt`
- [ ] å·²ç™»å½• HuggingFace `huggingface-cli login`
- [ ] å·²æµ‹è¯•è®¾å¤‡æ£€æµ‹ `python device_utils.py`
- [ ] å·²è¿è¡ŒåŸºç¡€æ¨ç†æµ‹è¯•
- [ ] è¾“å‡ºå›¾åƒè´¨é‡å¯æ¥å—
- [ ] æ€§èƒ½æ»¡è¶³éœ€æ±‚

ç¥ä½¿ç”¨é¡ºåˆ©ï¼å¦‚æœ‰é—®é¢˜ï¼Œè¯·å‚è€ƒ NPU_MIGRATION_GUIDE.md è·å–æ›´è¯¦ç»†çš„æŠ€æœ¯è¯´æ˜ã€‚

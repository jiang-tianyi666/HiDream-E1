"""
è®¾å¤‡ç®¡ç†å·¥å…· - æ”¯æŒ NPU/CUDA/CPU è‡ªåŠ¨åˆ‡æ¢
Device Management Utility - Auto-detection for NPU/CUDA/CPU
"""

import torch
import os
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DeviceManager:
    """ç»Ÿä¸€çš„è®¾å¤‡ç®¡ç†å™¨ï¼Œè‡ªåŠ¨æ£€æµ‹å¹¶é€‚é… NPU/CUDA/CPU"""

    def __init__(self, preferred_device=None):
        """
        åˆå§‹åŒ–è®¾å¤‡ç®¡ç†å™¨

        Args:
            preferred_device: å¯é€‰ï¼ŒæŒ‡å®šè®¾å¤‡ç±»å‹ 'npu', 'cuda', 'cpu'
                            å¦‚æœä¸º Noneï¼Œåˆ™è‡ªåŠ¨æ£€æµ‹
        """
        self.device_type = self._detect_device(preferred_device)
        self.device = self._get_device()
        self.dtype = self._get_dtype()

        logger.info(f"ğŸš€ Device Manager initialized")
        logger.info(f"   Device type: {self.device_type}")
        logger.info(f"   Device: {self.device}")
        logger.info(f"   Data type: {self.dtype}")

    def _detect_device(self, preferred=None):
        """è‡ªåŠ¨æ£€æµ‹å¯ç”¨è®¾å¤‡"""
        if preferred:
            preferred = preferred.lower()
            if preferred == 'npu':
                try:
                    import torch_npu
                    if torch.npu.is_available():
                        return 'npu'
                    else:
                        logger.warning("NPU requested but not available, falling back...")
                except ImportError:
                    logger.warning("torch_npu not installed, falling back...")
            elif preferred == 'cuda':
                if torch.cuda.is_available():
                    return 'cuda'
                else:
                    logger.warning("CUDA requested but not available, falling back...")
            elif preferred == 'cpu':
                return 'cpu'

        # è‡ªåŠ¨æ£€æµ‹é¡ºåºï¼šNPU > CUDA > CPU
        try:
            import torch_npu
            if torch.npu.is_available():
                logger.info("âœ“ NPU detected and available")
                return 'npu'
        except ImportError:
            pass

        if torch.cuda.is_available():
            logger.info("âœ“ CUDA detected and available")
            return 'cuda'

        logger.info("Using CPU (no GPU detected)")
        return 'cpu'

    def _get_device(self):
        """è·å–è®¾å¤‡å­—ç¬¦ä¸²"""
        if self.device_type == 'npu':
            try:
                import torch_npu
                device_id = int(os.environ.get('NPU_DEVICE_ID', torch.npu.current_device()))
                return f"npu:{device_id}"
            except:
                return "npu:0"
        elif self.device_type == 'cuda':
            device_id = int(os.environ.get('CUDA_VISIBLE_DEVICES', torch.cuda.current_device()))
            return f"cuda:{device_id}"
        else:
            return "cpu"

    def _get_dtype(self):
        """è·å–æ”¯æŒçš„æ•°æ®ç±»å‹ï¼Œä¼˜å…ˆä½¿ç”¨ bfloat16"""
        if self.device_type == 'cpu':
            # CPU é€šå¸¸ä¸æ”¯æŒ bfloat16 é«˜æ•ˆè¿ç®—
            logger.info("Using float32 for CPU")
            return torch.float32

        # æµ‹è¯• bfloat16 æ”¯æŒ
        try:
            test_tensor = torch.tensor([1.0], dtype=torch.bfloat16).to(self.device)
            _ = test_tensor * 2  # ç®€å•è¿ç®—æµ‹è¯•
            logger.info("âœ“ bfloat16 is supported")
            return torch.bfloat16
        except Exception as e:
            logger.warning(f"bfloat16 not supported ({e}), trying float16...")

            # å°è¯• float16
            try:
                test_tensor = torch.tensor([1.0], dtype=torch.float16).to(self.device)
                _ = test_tensor * 2
                logger.info("âœ“ Using float16 instead")
                return torch.float16
            except Exception as e:
                logger.warning(f"float16 not supported ({e}), using float32")
                return torch.float32

    def create_generator(self, seed):
        """
        åˆ›å»ºéšæœºæ•°ç”Ÿæˆå™¨

        Args:
            seed: éšæœºç§å­

        Returns:
            torch.Generator
        """
        try:
            generator = torch.Generator(self.device).manual_seed(int(seed))
            return generator
        except Exception as e:
            logger.warning(f"Failed to create generator on {self.device}: {e}")
            logger.warning("Falling back to CPU generator")
            return torch.Generator("cpu").manual_seed(int(seed))

    def memory_stats(self):
        """æ‰“å°å†…å­˜ç»Ÿè®¡ä¿¡æ¯"""
        if self.device_type == 'npu':
            try:
                import torch_npu
                allocated = torch.npu.memory_allocated() / 1024**3
                reserved = torch.npu.memory_reserved() / 1024**3
                max_allocated = torch.npu.max_memory_allocated() / 1024**3
                logger.info(f"ğŸ“Š NPU Memory Stats:")
                logger.info(f"   Allocated: {allocated:.2f} GB")
                logger.info(f"   Reserved: {reserved:.2f} GB")
                logger.info(f"   Peak: {max_allocated:.2f} GB")
            except Exception as e:
                logger.warning(f"Failed to get NPU memory stats: {e}")

        elif self.device_type == 'cuda':
            try:
                allocated = torch.cuda.memory_allocated() / 1024**3
                reserved = torch.cuda.memory_reserved() / 1024**3
                max_allocated = torch.cuda.max_memory_allocated() / 1024**3
                logger.info(f"ğŸ“Š CUDA Memory Stats:")
                logger.info(f"   Allocated: {allocated:.2f} GB")
                logger.info(f"   Reserved: {reserved:.2f} GB")
                logger.info(f"   Peak: {max_allocated:.2f} GB")
            except Exception as e:
                logger.warning(f"Failed to get CUDA memory stats: {e}")
        else:
            logger.info("ğŸ“Š CPU device - no GPU memory tracking")

    def empty_cache(self):
        """æ¸…ç©ºè®¾å¤‡ç¼“å­˜"""
        if self.device_type == 'npu':
            try:
                import torch_npu
                torch.npu.empty_cache()
                logger.info("âœ“ NPU cache cleared")
            except:
                pass
        elif self.device_type == 'cuda':
            torch.cuda.empty_cache()
            logger.info("âœ“ CUDA cache cleared")

    def set_memory_efficient_mode(self):
        """è®¾ç½®å†…å­˜é«˜æ•ˆæ¨¡å¼"""
        if self.device_type == 'npu':
            try:
                import torch_npu
                # NPU ç‰¹å®šä¼˜åŒ–
                torch_npu.npu.set_compile_mode(jit_compile=False)
                logger.info("âœ“ NPU memory efficient mode enabled")
            except Exception as e:
                logger.warning(f"Failed to set NPU memory mode: {e}")
        elif self.device_type == 'cuda':
            # CUDA ç‰¹å®šä¼˜åŒ–
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            logger.info("âœ“ CUDA TF32 mode enabled for better performance")

    def load_safetensors(self, path):
        """
        åŠ è½½ safetensors æ–‡ä»¶åˆ°æ­£ç¡®çš„è®¾å¤‡

        Args:
            path: safetensors æ–‡ä»¶è·¯å¾„

        Returns:
            state_dict
        """
        from safetensors.torch import load_file

        # å…ˆåŠ è½½åˆ° CPUï¼Œå†è½¬ç§»åˆ°ç›®æ ‡è®¾å¤‡ï¼ˆæ›´å®‰å…¨ï¼‰
        try:
            state_dict = load_file(path, device="cpu")
            logger.info(f"âœ“ Loaded {path} to CPU, will transfer to {self.device}")
            return state_dict
        except Exception as e:
            logger.error(f"Failed to load safetensors: {e}")
            raise

    def __repr__(self):
        return f"DeviceManager(device={self.device}, dtype={self.dtype})"


def get_device_manager(preferred_device=None):
    """
    è·å–å…¨å±€è®¾å¤‡ç®¡ç†å™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰

    Args:
        preferred_device: å¯é€‰ï¼ŒæŒ‡å®šè®¾å¤‡ç±»å‹

    Returns:
        DeviceManager instance
    """
    if not hasattr(get_device_manager, '_instance'):
        get_device_manager._instance = DeviceManager(preferred_device)
    return get_device_manager._instance


# ä¾¿æ·å‡½æ•°
def auto_device():
    """è¿”å›è‡ªåŠ¨æ£€æµ‹çš„è®¾å¤‡å­—ç¬¦ä¸²"""
    dm = get_device_manager()
    return dm.device


def auto_dtype():
    """è¿”å›è‡ªåŠ¨æ£€æµ‹çš„æ•°æ®ç±»å‹"""
    dm = get_device_manager()
    return dm.dtype


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("=" * 60)
    print("Device Manager Test")
    print("=" * 60)

    dm = DeviceManager()
    print(f"\n{dm}")

    # æµ‹è¯•ç”Ÿæˆå™¨
    print("\nğŸ“ Testing generator...")
    gen = dm.create_generator(42)
    print(f"âœ“ Generator created: {gen}")

    # æµ‹è¯•å†…å­˜ç»Ÿè®¡
    print("\nğŸ“Š Memory statistics:")
    dm.memory_stats()

    # æµ‹è¯•å¼ é‡åˆ›å»º
    print("\nğŸ§ª Testing tensor operations...")
    try:
        test_tensor = torch.randn(100, 100).to(dm.device, dm.dtype)
        result = torch.matmul(test_tensor, test_tensor.T)
        print(f"âœ“ Tensor operation successful")
        print(f"  Shape: {result.shape}, dtype: {result.dtype}, device: {result.device}")
    except Exception as e:
        print(f"âœ— Tensor operation failed: {e}")

    print("\n" + "=" * 60)
    print("Test completed!")
    print("=" * 60)

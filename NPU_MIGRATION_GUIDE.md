# åä¸ºæ˜‡è…¾ NPU è¿ç§»æŒ‡å—

æœ¬æŒ‡å—è¯´æ˜å¦‚ä½•å°† HiDream-E1 è¿ç§»åˆ°åä¸ºæ˜‡è…¾ NPU ä¸Šè¿è¡Œã€‚

## ğŸ” å…³é”®é—®é¢˜æ¦‚è§ˆ

å°†æ­¤æ¨¡å‹è¿ç§»åˆ°æ˜‡è…¾ NPU ä¸»è¦æ¶‰åŠä»¥ä¸‹æ–¹é¢ï¼š

### 1. **è®¾å¤‡æŒ‡å®šé—®é¢˜** (é«˜ä¼˜å…ˆçº§)
### 2. **Flash Attention é€‚é…** (å…³é”®)
### 3. **ç®—å­å…¼å®¹æ€§** (éœ€è¦æµ‹è¯•)
### 4. **ç²¾åº¦æ”¯æŒ** (bfloat16)
### 5. **ä¾èµ–åº“é€‚é…** (PyTorchã€Diffusers)

---

## ğŸ“‹ è¯¦ç»†è¿ç§»æ¸…å•

### ä¸€ã€ç¯å¢ƒå‡†å¤‡

#### 1.1 å®‰è£…æ˜‡è…¾ CANN å’Œ torch_npu

```bash
# 1. å®‰è£… CANN (å‡è®¾ä½¿ç”¨ CANN 8.0+)
# å‚è€ƒåä¸ºå®˜æ–¹æ–‡æ¡£å®‰è£… CANN toolkit å’Œ kernels

# 2. å®‰è£… torch å’Œ torch_npu
pip install torch==2.1.0  # æˆ–ä¸ä½ çš„ CANN ç‰ˆæœ¬åŒ¹é…çš„ torch
pip install torch_npu  # åä¸ºæä¾›çš„ NPU æ’ä»¶

# 3. éªŒè¯å®‰è£…
python -c "import torch; import torch_npu; print(torch.npu.is_available())"
```

#### 1.2 æ£€æŸ¥ NPU è®¾å¤‡

```python
import torch
import torch_npu

# æ£€æŸ¥ NPU æ•°é‡
print(f"NPU count: {torch.npu.device_count()}")

# è®¾ç½®é»˜è®¤ NPU
torch.npu.set_device(0)
```

---

### äºŒã€ä»£ç ä¿®æ”¹è¦ç‚¹

#### 2.1 è®¾å¤‡å­—ç¬¦ä¸²æ›¿æ¢ (å¿…é¡»)

**é—®é¢˜æ–‡ä»¶**ï¼š
- [inference_e1_1.py:107](inference_e1_1.py#L107)
- [inference.py:64](inference.py#L64)
- [gradio_demo_1_1.py:107](gradio_demo_1_1.py#L107)
- [gradio_demo.py:62](gradio_demo.py#L62)

**ä¿®æ”¹ç¤ºä¾‹**ï¼š

```python
# åŸä»£ç 
pipe = pipe.to("cuda", torch.bfloat16)

# ä¿®æ”¹ä¸º
device = "npu:0"  # æˆ–ä½¿ç”¨ç¯å¢ƒå˜é‡æ§åˆ¶
pipe = pipe.to(device, torch.bfloat16)
```

**å»ºè®®ä½¿ç”¨é…ç½®æ–¹å¼**ï¼š

```python
# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ 
import os

# è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
if torch.npu.is_available():
    DEVICE = f"npu:{torch.npu.current_device()}"
elif torch.cuda.is_available():
    DEVICE = "cuda"
else:
    DEVICE = "cpu"

print(f"Using device: {DEVICE}")

# åç»­ä½¿ç”¨
pipe = pipe.to(DEVICE, torch.bfloat16)
```

#### 2.2 Generator è®¾å¤‡ä¿®æ”¹ (å¿…é¡»)

**é—®é¢˜ä½ç½®**ï¼š
- [inference_e1_1.py:153](inference_e1_1.py#L153)
- [inference.py:88](inference.py#L88)
- [gradio_demo.py:79](gradio_demo.py#L79)
- [gradio_demo_1_1.py:125](gradio_demo_1_1.py#L125)

```python
# åŸä»£ç 
generator = torch.Generator("cuda").manual_seed(seed)

# ä¿®æ”¹ä¸º
generator = torch.Generator(DEVICE).manual_seed(seed)
```

#### 2.3 å†…å­˜ç›‘æ§é€‚é…

**é—®é¢˜ä½ç½®**ï¼š
- [inference_e1_1.py:110](inference_e1_1.py#L110)
- [gradio_demo_1_1.py:109](gradio_demo_1_1.py#L109)

```python
# åŸä»£ç 
torch.cuda.memory_summary(device='cuda', abbreviated=True)

# ä¿®æ”¹ä¸º
if DEVICE.startswith('npu'):
    # NPU å†…å­˜æŸ¥è¯¢
    memory_allocated = torch.npu.memory_allocated() / 1024**3  # GB
    memory_reserved = torch.npu.memory_reserved() / 1024**3
    print(f"NPU Memory: {memory_allocated:.2f}GB / {memory_reserved:.2f}GB")
else:
    torch.cuda.memory_summary(device=DEVICE, abbreviated=True)
```

#### 2.4 æƒé‡åŠ è½½è®¾å¤‡

**é—®é¢˜ä½ç½®**ï¼š
- [inference.py:38](inference.py#L38)
- [gradio_demo.py:34](gradio_demo.py#L34)

```python
# åŸä»£ç 
lora_ckpt = load_file(lora_ckpt_path, device="cuda")

# ä¿®æ”¹ä¸º
lora_ckpt = load_file(lora_ckpt_path, device=DEVICE)

# æˆ–å…ˆåŠ è½½åˆ° CPU å†è½¬ç§»
lora_ckpt = load_file(lora_ckpt_path, device="cpu")
# åç»­ä¼šè‡ªåŠ¨è½¬ç§»åˆ°æ­£ç¡®è®¾å¤‡
```

---

### ä¸‰ã€Flash Attention é€‚é… (å…³é”®)

#### 3.1 é—®é¢˜è¯´æ˜

åŸä»£ç ä¾èµ– CUDA Flash Attentionï¼š
```bash
pip install -U flash-attn --no-build-isolation
```

**æ˜‡è…¾ NPU ä¸æ”¯æŒ CUDA Flash Attention**ï¼Œéœ€è¦æ›¿ä»£æ–¹æ¡ˆã€‚

#### 3.2 è§£å†³æ–¹æ¡ˆ

**æ–¹æ¡ˆ Aï¼šä½¿ç”¨æ ‡å‡† Attentionï¼ˆç®€å•ä½†æ…¢ï¼‰**

ä¿®æ”¹ Diffusers åº“æˆ–åœ¨åˆå§‹åŒ–æ—¶ç¦ç”¨ Flash Attentionï¼š

```python
# åœ¨åŠ è½½æ¨¡å‹å‰è®¾ç½®
import os
os.environ["DIFFUSERS_ATTENTION_TYPE"] = "vanilla"

# æˆ–åœ¨ transformer é…ç½®ä¸­
transformer = HiDreamImageTransformer2DModel.from_pretrained(
    ...,
    use_flash_attention=False  # å¦‚æœæ¨¡å‹æ”¯æŒæ­¤å‚æ•°
)
```

**æ–¹æ¡ˆ Bï¼šä½¿ç”¨æ˜‡è…¾ä¼˜åŒ–çš„ Attention**

åä¸ºå¯èƒ½æä¾›äº†ä¼˜åŒ–çš„ attention å®ç°ï¼ˆéœ€è¦æŸ¥çœ‹ CANN æ–‡æ¡£ï¼‰ï¼š

```python
# å¯èƒ½éœ€è¦ä¿®æ”¹ diffusers æºç ä¸­çš„ attention å±‚
# è·¯å¾„ç±»ä¼¼ï¼šsite-packages/diffusers/models/attention_processor.py

from torch_npu.contrib.module import MultiHeadAttention  # å‡è®¾å­˜åœ¨

# æ›¿æ¢ attention å®ç°
```

**æ–¹æ¡ˆ Cï¼šç¼–è¯‘å®‰è£…æ˜‡è…¾ç‰ˆ Flash Attentionï¼ˆå¦‚æœå¯ç”¨ï¼‰**

æ£€æŸ¥åä¸ºæ˜¯å¦æä¾›äº† NPU ç‰ˆæœ¬çš„ Flash Attentionï¼š
```bash
# æŸ¥æ‰¾æ˜‡è…¾ç¤¾åŒºæ˜¯å¦æœ‰å®ç°
# æˆ–è”ç³»åä¸ºæŠ€æœ¯æ”¯æŒ
```

#### 3.3 æ€§èƒ½å½±å“

- æ ‡å‡† Attentionï¼šé€Ÿåº¦å¯èƒ½ä¸‹é™ 30-50%
- æ˜‡è…¾ä¼˜åŒ–ç‰ˆï¼šæ¥è¿‘ CUDA Flash Attention æ€§èƒ½

---

### å››ã€Diffusers åº“é€‚é…

#### 4.1 æ£€æŸ¥å…¼å®¹æ€§

```python
# æµ‹è¯• Diffusers åœ¨ NPU ä¸Šçš„åŸºæœ¬åŠŸèƒ½
import torch
import torch_npu
from diffusers import AutoencoderKL

device = "npu:0"
vae = AutoencoderKL.from_pretrained("stabilityai/sd-vae-ft-mse")
vae = vae.to(device, torch.bfloat16)

# æµ‹è¯•å‰å‘ä¼ æ’­
dummy_input = torch.randn(1, 3, 512, 512).to(device, torch.bfloat16)
try:
    latent = vae.encode(dummy_input).latent_dist.sample()
    print("VAE encoding test passed!")
except Exception as e:
    print(f"VAE test failed: {e}")
```

#### 4.2 å¯èƒ½éœ€è¦çš„è¡¥ä¸

å¦‚æœé‡åˆ°ç®—å­ä¸æ”¯æŒï¼Œå¯èƒ½éœ€è¦ï¼š

```python
# åœ¨æ¨¡å‹åŠ è½½å‰
import torch_npu
torch_npu.npu.set_compile_mode(jit_compile=False)  # ç¦ç”¨ JIT ä»¥è°ƒè¯•

# æˆ–è®¾ç½®ç®—å­å›é€€
os.environ['NPU_FALLBACK_MODE'] = '1'  # ä¸æ”¯æŒçš„ç®—å­å›é€€åˆ° CPU
```

---

### äº”ã€ç²¾åº¦é—®é¢˜

#### 5.1 bfloat16 æ”¯æŒæ£€æŸ¥

```python
# æ£€æŸ¥ NPU æ˜¯å¦æ”¯æŒ bfloat16
device = "npu:0"
try:
    test_tensor = torch.randn(10, 10, dtype=torch.bfloat16).to(device)
    print("bfloat16 is supported on NPU")
except Exception as e:
    print(f"bfloat16 not supported: {e}")
    print("Consider using float16 or float32")
```

#### 5.2 ç²¾åº¦é™çº§æ–¹æ¡ˆ

å¦‚æœ bfloat16 ä¸æ”¯æŒï¼š

```python
# ä½¿ç”¨ float16
DTYPE = torch.float16 if not torch.npu.is_bf16_supported() else torch.bfloat16

pipe = pipe.to(DEVICE, DTYPE)
```

---

### å…­ã€å®Œæ•´è¿ç§»ç¤ºä¾‹ä»£ç 

åˆ›å»ºä¸€ä¸ªé€šç”¨çš„è®¾å¤‡æŠ½è±¡å±‚ï¼š

```python
# device_utils.py
import torch
import os

class DeviceManager:
    def __init__(self):
        self.device_type = self._detect_device()
        self.device = self._get_device()
        self.dtype = self._get_dtype()

    def _detect_device(self):
        """è‡ªåŠ¨æ£€æµ‹å¯ç”¨è®¾å¤‡"""
        try:
            import torch_npu
            if torch.npu.is_available():
                return 'npu'
        except ImportError:
            pass

        if torch.cuda.is_available():
            return 'cuda'

        return 'cpu'

    def _get_device(self):
        """è·å–è®¾å¤‡å­—ç¬¦ä¸²"""
        if self.device_type == 'npu':
            return f"npu:{torch.npu.current_device()}"
        elif self.device_type == 'cuda':
            return f"cuda:{torch.cuda.current_device()}"
        else:
            return "cpu"

    def _get_dtype(self):
        """è·å–æ”¯æŒçš„æ•°æ®ç±»å‹"""
        if self.device_type == 'npu':
            # æ£€æŸ¥ bfloat16 æ”¯æŒ
            try:
                test = torch.tensor([1.0], dtype=torch.bfloat16).to(self.device)
                return torch.bfloat16
            except:
                print("Warning: bfloat16 not supported on NPU, using float16")
                return torch.float16
        else:
            return torch.bfloat16

    def create_generator(self, seed):
        """åˆ›å»ºéšæœºç”Ÿæˆå™¨"""
        return torch.Generator(self.device).manual_seed(seed)

    def memory_stats(self):
        """æ‰“å°å†…å­˜ç»Ÿè®¡"""
        if self.device_type == 'npu':
            allocated = torch.npu.memory_allocated() / 1024**3
            reserved = torch.npu.memory_reserved() / 1024**3
            print(f"NPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved")
        elif self.device_type == 'cuda':
            print(torch.cuda.memory_summary(abbreviated=True))
        else:
            print("CPU device - no GPU memory tracking")

# ä½¿ç”¨ç¤ºä¾‹
dm = DeviceManager()
print(f"Using device: {dm.device} with dtype: {dm.dtype}")

# åœ¨æ¨¡å‹åŠ è½½æ—¶ä½¿ç”¨
pipe = pipe.to(dm.device, dm.dtype)
generator = dm.create_generator(seed=42)
```

---

### ä¸ƒã€æµ‹è¯•å’ŒéªŒè¯

#### 7.1 é€æ­¥æµ‹è¯•

```bash
# 1. æµ‹è¯•åŸºç¡€ç¯å¢ƒ
python -c "import torch; import torch_npu; print(torch.npu.is_available())"

# 2. æµ‹è¯•æ–‡æœ¬ç¼–ç å™¨
python test_text_encoders_npu.py

# 3. æµ‹è¯• VAE
python test_vae_npu.py

# 4. æµ‹è¯•å®Œæ•´æ¨ç†
python inference_e1_1_npu.py
```

#### 7.2 æ€§èƒ½å¯¹æ¯”

```python
import time

# è®°å½•æ¨ç†æ—¶é—´
start = time.time()
image = pipe(
    prompt=instruction,
    image=test_image,
    num_inference_steps=28,
    ...
).images[0]
end = time.time()

print(f"Inference time: {end - start:.2f}s")
dm.memory_stats()
```

---

### å…«ã€å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

#### 8.1 ç®—å­ä¸æ”¯æŒ

**é”™è¯¯**: `RuntimeError: operator XXX is not implemented for NPU`

**è§£å†³**:
```python
# æ–¹æ¡ˆ1: å›é€€åˆ°CPU
os.environ['NPU_FALLBACK_MODE'] = '1'

# æ–¹æ¡ˆ2: æŸ¥æ‰¾æ›¿ä»£ç®—å­
# è”ç³»åä¸ºæŠ€æœ¯æ”¯æŒè·å–ç®—å­æ”¯æŒåˆ—è¡¨
```

#### 8.2 ç²¾åº¦é—®é¢˜

**ç°è±¡**: è¾“å‡ºå›¾åƒè´¨é‡æ˜æ˜¾ä¸‹é™

**æ£€æŸ¥**:
```python
# ç¡®è®¤ä½¿ç”¨çš„ç²¾åº¦
print(f"Model dtype: {pipe.transformer.dtype}")
print(f"VAE dtype: {pipe.vae.dtype}")

# å°è¯•æ··åˆç²¾åº¦
pipe.vae = pipe.vae.to(torch.float32)  # VAE ç”¨ fp32
```

#### 8.3 å†…å­˜æº¢å‡º

**è§£å†³**:
```python
# å¯ç”¨ CPU offload
pipe.enable_model_cpu_offload()

# æˆ–å¯ç”¨åºåˆ—åŒ– CPU offload
pipe.enable_sequential_cpu_offload()

# å‡å°‘ batch size æˆ–åˆ†è¾¨ç‡
```

#### 8.4 Transformers åº“ç‰ˆæœ¬å…¼å®¹

```bash
# å¯èƒ½éœ€è¦ç‰¹å®šç‰ˆæœ¬çš„ transformers
pip install transformers==4.36.0  # æµ‹è¯•ä¸åŒç‰ˆæœ¬

# æ£€æŸ¥ Llama æ¨¡å‹åŠ è½½
python -c "from transformers import LlamaForCausalLM; print('OK')"
```

---

### ä¹ã€æ€§èƒ½ä¼˜åŒ–å»ºè®®

#### 9.1 ä½¿ç”¨å›¾ç¼–è¯‘ï¼ˆå¦‚æœæ”¯æŒï¼‰

```python
# å°è¯•ä½¿ç”¨æ˜‡è…¾çš„å›¾ç¼–è¯‘åŠŸèƒ½
import torch_npu
from torch_npu.contrib import transfer_to_npu

# å¯èƒ½éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œè¿½è¸ªç¼–è¯‘
# å‚è€ƒ CANN æ–‡æ¡£çš„å›¾ç¼–è¯‘éƒ¨åˆ†
```

#### 9.2 æ··åˆç²¾åº¦è®­ç»ƒ/æ¨ç†

```python
from torch.cuda.amp import autocast  # æˆ– torch_npu çš„ç­‰æ•ˆå®ç°

with autocast('npu'):  # å¯èƒ½éœ€è¦ä½¿ç”¨ torch_npu çš„ API
    output = pipe(...)
```

#### 9.3 æ‰¹å¤„ç†ä¼˜åŒ–

```python
# å¦‚æœå¤„ç†å¤šå¼ å›¾ï¼Œä½¿ç”¨æ‰¹å¤„ç†
images = [img1, img2, img3]
prompts = ["edit1", "edit2", "edit3"]

# æ‰¹é‡æ¨ç†
outputs = pipe(
    prompt=prompts,
    image=images,
    ...
)
```

---

### åã€æ¨èè¿ç§»æ­¥éª¤

1. **ç¯å¢ƒå‡†å¤‡** (1-2å¤©)
   - å®‰è£… CANNã€torch_npu
   - éªŒè¯åŸºç¡€åŠŸèƒ½

2. **ä»£ç åŸºç¡€é€‚é…** (1å¤©)
   - æ›¿æ¢æ‰€æœ‰ "cuda" ä¸ºè®¾å¤‡å˜é‡
   - ä½¿ç”¨ DeviceManager ç»Ÿä¸€ç®¡ç†

3. **ä¾èµ–åº“æµ‹è¯•** (2-3å¤©)
   - æµ‹è¯• Diffusers å…¼å®¹æ€§
   - æµ‹è¯• Transformers Llama æ¨¡å‹
   - è§£å†³ Flash Attention é—®é¢˜

4. **åŠŸèƒ½éªŒè¯** (2å¤©)
   - ç«¯åˆ°ç«¯æ¨ç†æµ‹è¯•
   - å¯¹æ¯”è¾“å‡ºè´¨é‡
   - æ€§èƒ½åŸºå‡†æµ‹è¯•

5. **ä¼˜åŒ–è°ƒä¼˜** (æŒ‰éœ€)
   - æ€§èƒ½ä¼˜åŒ–
   - å†…å­˜ä¼˜åŒ–
   - æ‰¹å¤„ç†ä¼˜åŒ–

---

### åä¸€ã€è”ç³»æ”¯æŒ

- **åä¸ºæ˜‡è…¾ç¤¾åŒº**: https://www.hiascend.com/forum
- **CANN æ–‡æ¡£**: https://www.hiascend.com/document
- **PyTorch NPU æ’ä»¶**: https://gitee.com/ascend/pytorch

---

## ğŸ“ æ£€æŸ¥æ¸…å•

è¿ç§»å‰è¯·ç¡®è®¤ï¼š

- [ ] å·²å®‰è£… CANN toolkit å’Œ kernels
- [ ] å·²å®‰è£…åŒ¹é…ç‰ˆæœ¬çš„ torch å’Œ torch_npu
- [ ] NPU è®¾å¤‡å¯æ­£å¸¸è¯†åˆ« (`torch.npu.is_available()`)
- [ ] ç¡®è®¤ bfloat16/float16 æ”¯æŒæƒ…å†µ
- [ ] å‡†å¤‡å¥½æµ‹è¯•å›¾ç‰‡å’Œå¯¹æ¯”åŸºå‡†
- [ ] äº†è§£ Flash Attention çš„æ›¿ä»£æ–¹æ¡ˆ
- [ ] å¤‡ä»½åŸå§‹ CUDA ä»£ç 

è¿ç§»åè¯·éªŒè¯ï¼š

- [ ] æ¨¡å‹èƒ½æˆåŠŸåŠ è½½åˆ° NPU
- [ ] æ¨ç†èƒ½æ­£å¸¸å®Œæˆï¼ˆè‡³å°‘ä½¿ç”¨ float32ï¼‰
- [ ] è¾“å‡ºè´¨é‡ä¸ CUDA ç‰ˆæœ¬å¯¹æ¯”å¯æ¥å—
- [ ] å†…å­˜ä½¿ç”¨åœ¨å¯æ§èŒƒå›´
- [ ] æ¨ç†é€Ÿåº¦æ»¡è¶³éœ€æ±‚

---

## âš ï¸ é‡è¦æç¤º

1. **ä¸è¦æœŸæœ›é›¶ä¿®æ”¹è¿è¡Œ** - NPU å’Œ CUDA æœ‰æœ¬è´¨åŒºåˆ«
2. **ä»ç®€å•åˆ°å¤æ‚** - å…ˆæµ‹è¯•å•ä¸ªç»„ä»¶å†æµ‹è¯•å®Œæ•´æµç¨‹
3. **ä¿ç•™ CUDA ç‰ˆæœ¬** - ä½œä¸ºåŠŸèƒ½å’Œæ€§èƒ½å¯¹æ¯”åŸºå‡†
4. **å…³æ³¨ç®—å­è¦†ç›–ç‡** - æŸäº›é«˜çº§ç®—å­å¯èƒ½ä¸æ”¯æŒ
5. **æ€§èƒ½å¯èƒ½ä¸åŒ** - é¦–æ¬¡è¿ç§»é‡ç‚¹æ˜¯åŠŸèƒ½ï¼Œæ€§èƒ½éœ€è¦åç»­ä¼˜åŒ–

ç¥è¿ç§»é¡ºåˆ©ï¼

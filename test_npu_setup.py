#!/usr/bin/env python
"""
NPU ç¯å¢ƒæµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯ NPU ç¯å¢ƒæ˜¯å¦æ­£ç¡®é…ç½®
"""

import sys
import torch

def test_pytorch():
    """æµ‹è¯• PyTorch å®‰è£…"""
    print("=" * 60)
    print("1. Testing PyTorch Installation")
    print("=" * 60)
    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"  CUDA version: {torch.version.cuda}")
        print(f"  GPU count: {torch.cuda.device_count()}")
    print()

def test_npu():
    """æµ‹è¯• NPU å®‰è£…å’Œå¯ç”¨æ€§"""
    print("=" * 60)
    print("2. Testing NPU Installation")
    print("=" * 60)
    try:
        import torch_npu
        print("âœ“ torch_npu imported successfully")
        print(f"  torch_npu version: {torch_npu.__version__ if hasattr(torch_npu, '__version__') else 'unknown'}")

        npu_available = torch.npu.is_available()
        print(f"  NPU available: {npu_available}")

        if npu_available:
            print(f"  NPU count: {torch.npu.device_count()}")
            print(f"  Current NPU: {torch.npu.current_device()}")

            # æµ‹è¯• NPU åŸºæœ¬æ“ä½œ
            try:
                test_tensor = torch.randn(10, 10).to("npu:0")
                result = test_tensor @ test_tensor.T
                print("  âœ“ Basic NPU operations work")
            except Exception as e:
                print(f"  âœ— NPU operations failed: {e}")
        else:
            print("  âš ï¸  NPU is not available!")

    except ImportError:
        print("âœ— torch_npu not installed")
        print("  Please install: pip install torch_npu")
    print()

def test_device_manager():
    """æµ‹è¯•è®¾å¤‡ç®¡ç†å™¨"""
    print("=" * 60)
    print("3. Testing Device Manager")
    print("=" * 60)
    try:
        from device_utils import DeviceManager

        dm = DeviceManager()
        print(f"âœ“ DeviceManager initialized")
        print(f"  Device type: {dm.device_type}")
        print(f"  Device: {dm.device}")
        print(f"  Data type: {dm.dtype}")

        # æµ‹è¯•ç”Ÿæˆå™¨
        gen = dm.create_generator(42)
        print(f"  âœ“ Generator created: {type(gen).__name__}")

        # æµ‹è¯•å¼ é‡æ“ä½œ
        test_tensor = torch.randn(100, 100).to(dm.device, dm.dtype)
        result = test_tensor @ test_tensor.T
        print(f"  âœ“ Tensor operations work (shape: {result.shape})")

        # å†…å­˜ç»Ÿè®¡
        print("\n  Memory Stats:")
        dm.memory_stats()

    except ImportError as e:
        print(f"âœ— Failed to import device_utils: {e}")
    except Exception as e:
        print(f"âœ— DeviceManager test failed: {e}")
        import traceback
        traceback.print_exc()
    print()

def test_dependencies():
    """æµ‹è¯•å…³é”®ä¾èµ–"""
    print("=" * 60)
    print("4. Testing Dependencies")
    print("=" * 60)

    dependencies = [
        ("transformers", "Transformers"),
        ("diffusers", "Diffusers"),
        ("PIL", "Pillow"),
        ("safetensors", "SafeTensors"),
        ("peft", "PEFT"),
        ("einops", "Einops"),
        ("accelerate", "Accelerate"),
    ]

    for module_name, display_name in dependencies:
        try:
            module = __import__(module_name)
            version = getattr(module, "__version__", "unknown")
            print(f"âœ“ {display_name:15s} {version}")
        except ImportError:
            print(f"âœ— {display_name:15s} NOT INSTALLED")
    print()

def test_precision_support():
    """æµ‹è¯•ç²¾åº¦æ”¯æŒ"""
    print("=" * 60)
    print("5. Testing Precision Support")
    print("=" * 60)

    try:
        from device_utils import DeviceManager
        dm = DeviceManager()
        device = dm.device

        precisions = [
            (torch.float32, "float32"),
            (torch.float16, "float16"),
            (torch.bfloat16, "bfloat16"),
        ]

        for dtype, name in precisions:
            try:
                test = torch.tensor([1.0, 2.0, 3.0], dtype=dtype).to(device)
                result = test * 2
                print(f"âœ“ {name:10s} supported")
            except Exception as e:
                print(f"âœ— {name:10s} not supported: {e}")

    except Exception as e:
        print(f"âœ— Precision test failed: {e}")
    print()

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½ï¼ˆç®€åŒ–æµ‹è¯•ï¼‰"""
    print("=" * 60)
    print("6. Testing Model Loading (Basic)")
    print("=" * 60)

    try:
        from transformers import AutoTokenizer

        print("Testing tokenizer download...")
        # ä½¿ç”¨ä¸€ä¸ªå°æ¨¡å‹æµ‹è¯•
        tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
        print("âœ“ Model download and loading works")

    except Exception as e:
        print(f"âš ï¸  Model loading test failed: {e}")
        print("   This may be due to network issues or missing HuggingFace login")
    print()

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "=" * 60)
    print(" NPU Environment Test Suite")
    print("=" * 60 + "\n")

    test_pytorch()
    test_npu()
    test_device_manager()
    test_dependencies()
    test_precision_support()
    test_model_loading()

    print("=" * 60)
    print(" Test Summary")
    print("=" * 60)

    # åˆ¤æ–­æ˜¯å¦å¯ä»¥ä½¿ç”¨ NPU
    try:
        import torch_npu
        if torch.npu.is_available():
            print("âœ… NPU is ready to use!")
            print("   You can run: python inference_e1_1_npu.py")
        else:
            print("âš ï¸  NPU is installed but not available")
            print("   Check your CANN installation and NPU drivers")
    except ImportError:
        print("âš ï¸  torch_npu not installed")
        print("   Install it from: https://gitee.com/ascend/pytorch")

    # åˆ¤æ–­æ˜¯å¦å¯ä»¥ä½¿ç”¨ CUDA ä½œä¸ºæ›¿ä»£
    if torch.cuda.is_available():
        print("\nğŸ’¡ CUDA is available as fallback")
        print("   Set: export PREFERRED_DEVICE=cuda")

    print("\n" + "=" * 60)
    print(" For detailed NPU migration guide, see:")
    print("   - NPU_MIGRATION_GUIDE.md")
    print("   - README_NPU.md")
    print("=" * 60 + "\n")

if __name__ == "__main__":
    main()

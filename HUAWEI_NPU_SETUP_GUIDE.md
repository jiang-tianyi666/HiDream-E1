# åä¸ºæ˜‡è…¾æœåŠ¡å™¨éƒ¨ç½²å®Œæ•´æŒ‡å—

æœ¬æŒ‡å—æä¾›åœ¨åä¸ºæ˜‡è…¾ NPU æœåŠ¡å™¨ä¸Šéƒ¨ç½²å’Œè¿è¡Œ HiDream-E1 çš„è¯¦ç»†æ­¥éª¤ã€‚

---

## ğŸ“‹ ç›®å½•

1. [ç¯å¢ƒæ£€æŸ¥](#1-ç¯å¢ƒæ£€æŸ¥)
2. [å®‰è£… CANN](#2-å®‰è£…-cann)
3. [å®‰è£… PyTorch å’Œ torch_npu](#3-å®‰è£…-pytorch-å’Œ-torch_npu)
4. [éƒ¨ç½²é¡¹ç›®ä»£ç ](#4-éƒ¨ç½²é¡¹ç›®ä»£ç )
5. [å®‰è£…é¡¹ç›®ä¾èµ–](#5-å®‰è£…é¡¹ç›®ä¾èµ–)
6. [é…ç½® HuggingFace](#6-é…ç½®-huggingface)
7. [è¿è¡Œæµ‹è¯•](#7-è¿è¡Œæµ‹è¯•)
8. [æ‰§è¡Œæ¨ç†](#8-æ‰§è¡Œæ¨ç†)
9. [å¸¸è§é—®é¢˜](#9-å¸¸è§é—®é¢˜)
10. [æ€§èƒ½ä¼˜åŒ–](#10-æ€§èƒ½ä¼˜åŒ–)

---

## 1. ç¯å¢ƒæ£€æŸ¥

### 1.1 æ£€æŸ¥æœåŠ¡å™¨ä¿¡æ¯

```bash
# SSH ç™»å½•åˆ°åä¸ºæœåŠ¡å™¨
ssh user@your-huawei-server-ip

# æ£€æŸ¥æ“ä½œç³»ç»Ÿ
cat /etc/os-release
# æ¨è: Ubuntu 20.04/22.04 æˆ– CentOS 7/8

# æ£€æŸ¥ NPU è®¾å¤‡
npu-smi info
# åº”è¯¥èƒ½çœ‹åˆ° NPU è®¾å¤‡åˆ—è¡¨ï¼Œä¾‹å¦‚ Ascend 910 æˆ– 310P
```

**é¢„æœŸè¾“å‡ºç¤ºä¾‹ï¼š**
```
+-----------------------------------------------------------------------------+
| npu-smi 22.0.0                   Version: 22.0.0                            |
+-----------------------------------------------------------------------------+
| NPU     Name                   | Health | Power(W)    Temp(C)               |
| Chip                           | Bus-Id | AICore(%)   Memory-Usage(MB)      |
+=============================================================================+
| 0       Ascend910              | OK     | 120.0       45                    |
|         0                      | 0000:C1:00.0 | 0          0    / 32768        |
+=============================================================================+
```

### 1.2 æ£€æŸ¥ CANN æ˜¯å¦å·²å®‰è£…

```bash
# æ£€æŸ¥ CANN ç‰ˆæœ¬
cat /usr/local/Ascend/ascend-toolkit/latest/version.info

# æˆ–è€…
ls /usr/local/Ascend/
```

**å¦‚æœçœ‹åˆ°ç±»ä¼¼è¾“å‡ºï¼Œè¯´æ˜ CANN å·²å®‰è£…ï¼š**
```
/usr/local/Ascend/
â”œâ”€â”€ ascend-toolkit/
â”œâ”€â”€ driver/
â””â”€â”€ nnae/
```

---

## 2. å®‰è£… CANN

å¦‚æœ CANN æœªå®‰è£…ï¼Œéœ€è¦å…ˆå®‰è£…ã€‚

### 2.1 ä¸‹è½½ CANN

è®¿é—®åä¸ºå®˜ç½‘ä¸‹è½½ CANNï¼š
- **ä¸‹è½½åœ°å€**: https://www.hiascend.com/software/cann/community

é€‰æ‹©ç‰ˆæœ¬ï¼š
- **æ¨è**: CANN 8.0.RC1 æˆ–æ›´é«˜ç‰ˆæœ¬
- **æ¶æ„**: æ ¹æ®ä½ çš„ NPU å‹å·é€‰æ‹©ï¼ˆAscend 910/310P ç­‰ï¼‰
- **æ“ä½œç³»ç»Ÿ**: åŒ¹é…ä½ çš„æœåŠ¡å™¨ç³»ç»Ÿ

### 2.2 å®‰è£… CANN Toolkit

```bash
# å‡è®¾ä¸‹è½½çš„æ–‡ä»¶åä¸º Ascend-cann-toolkit_8.0.RC1_linux-*.run
chmod +x Ascend-cann-toolkit_8.0.RC1_linux-*.run

# å®‰è£…ï¼ˆéœ€è¦ root æƒé™æˆ– sudoï¼‰
sudo ./Ascend-cann-toolkit_8.0.RC1_linux-*.run --install

# æŒ‰æç¤ºé€‰æ‹©å®‰è£…è·¯å¾„ï¼Œé»˜è®¤: /usr/local/Ascend
```

### 2.3 å®‰è£… CANN Kernels

```bash
# ä¸‹è½½å¹¶å®‰è£… kernelsï¼ˆå¿…éœ€ï¼‰
chmod +x Ascend-cann-kernels-910_8.0.RC1_linux.run
sudo ./Ascend-cann-kernels-910_8.0.RC1_linux.run --install
```

### 2.4 é…ç½®ç¯å¢ƒå˜é‡

```bash
# ç¼–è¾‘ ~/.bashrc
vim ~/.bashrc

# æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼ˆæ ¹æ®å®é™…å®‰è£…è·¯å¾„è°ƒæ•´ï¼‰
# ==================== CANN ç¯å¢ƒå˜é‡ ====================
export ASCEND_HOME=/usr/local/Ascend
export ASCEND_TOOLKIT_HOME=${ASCEND_HOME}/ascend-toolkit/latest

# CANN è·¯å¾„
source ${ASCEND_TOOLKIT_HOME}/set_env.sh

# NPU è®¾å¤‡å¯è§æ€§ï¼ˆç±»ä¼¼ CUDA_VISIBLE_DEVICESï¼‰
export NPU_VISIBLE_DEVICES=0  # ä½¿ç”¨ç¬¬ä¸€ä¸ª NPU

# æ—¥å¿—çº§åˆ«ï¼ˆå¯é€‰ï¼Œè°ƒè¯•æ—¶ä½¿ç”¨ï¼‰
export ASCEND_GLOBAL_LOG_LEVEL=3  # 3=INFO, 0=DEBUG
export ASCEND_SLOG_PRINT_TO_STDOUT=1  # æ‰“å°æ—¥å¿—åˆ°ç»ˆç«¯
# ======================================================

# ä¿å­˜å¹¶ç”Ÿæ•ˆ
source ~/.bashrc
```

### 2.5 éªŒè¯ CANN å®‰è£…

```bash
# æ£€æŸ¥ç¯å¢ƒå˜é‡
echo $ASCEND_TOOLKIT_HOME

# æµ‹è¯• npu-smi
npu-smi info

# æ£€æŸ¥åº“æ–‡ä»¶
ls $ASCEND_TOOLKIT_HOME/lib64/
# åº”è¯¥çœ‹åˆ° libascendcl.so, libge_runner.so ç­‰
```

---

## 3. å®‰è£… PyTorch å’Œ torch_npu

### 3.1 åˆ›å»º Python è™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰

```bash
# ä½¿ç”¨ condaï¼ˆæ¨èï¼‰
conda create -n hidream python=3.10
conda activate hidream

# æˆ–ä½¿ç”¨ venv
python3 -m venv ~/hidream_env
source ~/hidream_env/bin/activate
```

### 3.2 ç¡®å®š PyTorch å’Œ torch_npu ç‰ˆæœ¬

**å…³é”®**: PyTorch ç‰ˆæœ¬å¿…é¡»ä¸ CANN ç‰ˆæœ¬åŒ¹é…ï¼

| CANN ç‰ˆæœ¬ | PyTorch ç‰ˆæœ¬ | torch_npu ç‰ˆæœ¬ |
|-----------|--------------|----------------|
| 8.0.RC1   | 2.1.0        | å¯¹åº”ç‰ˆæœ¬       |
| 7.0.0     | 2.0.1        | å¯¹åº”ç‰ˆæœ¬       |
| 6.3.0     | 1.11.0       | å¯¹åº”ç‰ˆæœ¬       |

æŸ¥çœ‹å®˜æ–¹å…¼å®¹æ€§æ–‡æ¡£ï¼š
https://www.hiascend.com/document/detail/zh/Pytorch/60RC1/configandinstg/instg/instg_0001.html

### 3.3 å®‰è£… PyTorch

```bash
# ç¤ºä¾‹ï¼šå®‰è£… PyTorch 2.1.0ï¼ˆæ ¹æ®ä½ çš„ CANN ç‰ˆæœ¬è°ƒæ•´ï¼‰
pip install torch==2.1.0 torchvision==0.16.0 --index-url https://download.pytorch.org/whl/cpu

# éªŒè¯å®‰è£…
python -c "import torch; print(f'PyTorch version: {torch.__version__}')"
```

### 3.4 å®‰è£… torch_npu

**æ–¹æ³• A: ä½¿ç”¨ pipï¼ˆæ¨èï¼‰**

```bash
# æŸ¥çœ‹å¯ç”¨ç‰ˆæœ¬
# https://gitee.com/ascend/pytorch/releases

# å®‰è£…ï¼ˆæ ¹æ® PyTorch ç‰ˆæœ¬é€‰æ‹©å¯¹åº”çš„ torch_npuï¼‰
pip install torch_npu

# æˆ–æŒ‡å®šç‰ˆæœ¬
# pip install torch-npu==2.1.0.post3
```

**æ–¹æ³• B: ä»æºç å®‰è£…**

```bash
# å…‹éš†ä»“åº“
git clone https://gitee.com/ascend/pytorch.git
cd pytorch

# åˆ‡æ¢åˆ°å¯¹åº”åˆ†æ”¯
git checkout v2.1.0-5.0.0  # æ ¹æ®å®é™…ç‰ˆæœ¬

# ç¼–è¯‘å®‰è£…
bash ci/build.sh --python=3.10
pip install dist/torch_npu-*.whl
```

### 3.5 éªŒè¯ torch_npu å®‰è£…

```bash
# æµ‹è¯•è„šæœ¬
python << 'EOF'
import torch
import torch_npu

print(f"PyTorch version: {torch.__version__}")
print(f"torch_npu version: {torch_npu.__version__ if hasattr(torch_npu, '__version__') else 'unknown'}")

# æ£€æŸ¥ NPU å¯ç”¨æ€§
if torch.npu.is_available():
    print(f"âœ“ NPU is available!")
    print(f"  NPU count: {torch.npu.device_count()}")
    print(f"  Current NPU: {torch.npu.current_device()}")
    print(f"  NPU name: {torch.npu.get_device_name(0)}")

    # æµ‹è¯•åŸºæœ¬è¿ç®—
    x = torch.randn(3, 3).to("npu:0")
    y = x @ x.T
    print(f"  âœ“ Basic tensor operations work")
else:
    print("âœ— NPU is not available!")
    print("  Check CANN installation and environment variables")
EOF
```

**é¢„æœŸè¾“å‡ºï¼š**
```
PyTorch version: 2.1.0
torch_npu version: 2.1.0.post3
âœ“ NPU is available!
  NPU count: 1
  Current NPU: 0
  NPU name: Ascend910
  âœ“ Basic tensor operations work
```

---

## 4. éƒ¨ç½²é¡¹ç›®ä»£ç 

### 4.1 ä¸Šä¼ ä»£ç åˆ°æœåŠ¡å™¨

**æ–¹æ³• A: ä½¿ç”¨ git**

```bash
# åœ¨æœåŠ¡å™¨ä¸Š
cd ~
git clone https://github.com/your-repo/HiDream-E1.git
cd HiDream-E1
```

**æ–¹æ³• B: ä½¿ç”¨ scp**

```bash
# åœ¨æœ¬åœ°æœºå™¨ä¸Š
scp -r /Users/jty/File/MLLM/HiDream-E1 user@server-ip:~/
```

**æ–¹æ³• C: ä½¿ç”¨ rsyncï¼ˆæ¨èï¼Œæ”¯æŒå¢é‡åŒæ­¥ï¼‰**

```bash
# åœ¨æœ¬åœ°æœºå™¨ä¸Š
rsync -avz --progress \
  /Users/jty/File/MLLM/HiDream-E1/ \
  user@server-ip:~/HiDream-E1/
```

### 4.2 æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§

```bash
cd ~/HiDream-E1
ls -lh

# ç¡®ä¿æœ‰ä»¥ä¸‹ NPU ç›¸å…³æ–‡ä»¶
ls -1 | grep -E "(npu|device_utils)"
```

**åº”è¯¥çœ‹åˆ°ï¼š**
```
device_utils.py
gradio_demo_npu.py
inference_e1_1_npu.py
inference_npu.py
npu_attention_optimizer.py
requirements_npu.txt
README_NPU.md
NPU_MIGRATION_GUIDE.md
FLASH_ATTENTION_SOLUTIONS.md
test_npu_setup.py
run_npu.sh
```

---

## 5. å®‰è£…é¡¹ç›®ä¾èµ–

### 5.1 å®‰è£…åŸºç¡€ä¾èµ–

```bash
cd ~/HiDream-E1

# ç¡®ä¿è™šæ‹Ÿç¯å¢ƒå·²æ¿€æ´»
conda activate hidream  # æˆ– source ~/hidream_env/bin/activate

# å®‰è£… NPU ç‰ˆæœ¬ä¾èµ–
pip install -r requirements_npu.txt

# è¿™ä¸ªè¿‡ç¨‹å¯èƒ½éœ€è¦ 5-10 åˆ†é’Ÿ
```

### 5.2 å¤„ç†å¸¸è§å®‰è£…é—®é¢˜

**é—®é¢˜ 1: Diffusers å®‰è£…å¤±è´¥**
```bash
# å¦‚æœä» git å®‰è£…å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ PyPI ç‰ˆæœ¬
pip install diffusers>=0.27.0
```

**é—®é¢˜ 2: Transformers ç‰ˆæœ¬å†²çª**
```bash
pip install transformers==4.47.1 --force-reinstall
```

**é—®é¢˜ 3: ç¼ºå°‘ç³»ç»Ÿåº“**
```bash
# Ubuntu/Debian
sudo apt-get update
sudo apt-get install -y build-essential python3-dev

# CentOS/RHEL
sudo yum install -y gcc gcc-c++ python3-devel
```

### 5.3 éªŒè¯ä¾èµ–å®‰è£…

```bash
# è¿è¡Œæµ‹è¯•è„šæœ¬
python test_npu_setup.py
```

**é¢„æœŸçœ‹åˆ°ï¼š**
```
==============================================================
 NPU Environment Test Suite
==============================================================

1. Testing PyTorch Installation
==============================================================
PyTorch version: 2.1.0
CUDA available: False

2. Testing NPU Installation
==============================================================
âœ“ torch_npu imported successfully
  NPU available: True
  NPU count: 1
  Current NPU: 0
  âœ“ Basic NPU operations work

3. Testing Device Manager
==============================================================
âœ“ DeviceManager initialized
  Device type: npu
  Device: npu:0
  Data type: torch.bfloat16
  ...
```

---

## 6. é…ç½® HuggingFace

### 6.1 ç™»å½• HuggingFace

```bash
# å®‰è£… CLIï¼ˆå¦‚æœæœªå®‰è£…ï¼‰
pip install -U huggingface_hub

# ç™»å½•
huggingface-cli login
```

**æŒ‰æç¤ºè¾“å…¥ä½ çš„ HuggingFace Tokenï¼š**
1. è®¿é—® https://huggingface.co/settings/tokens
2. åˆ›å»ºæˆ–å¤åˆ¶ Access Token
3. ç²˜è´´åˆ°ç»ˆç«¯

### 6.2 åŒæ„ Llama æ¨¡å‹åè®®

è®¿é—®å¹¶åŒæ„åè®®ï¼š
https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct

### 6.3 éªŒè¯ç™»å½•

```bash
huggingface-cli whoami
# åº”è¯¥æ˜¾ç¤ºä½ çš„ç”¨æˆ·å

# æµ‹è¯•ä¸‹è½½ï¼ˆå¯é€‰ï¼Œä¼šå ç”¨ç©ºé—´ï¼‰
python -c "
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
print('âœ“ HuggingFace works')
"
```

### 6.4 é…ç½®ç¼“å­˜ç›®å½•ï¼ˆå¯é€‰ï¼‰

```bash
# å¦‚æœä¸»ç›®å½•ç©ºé—´ä¸è¶³ï¼Œè®¾ç½®ç¼“å­˜åˆ°å…¶ä»–ä½ç½®
export HF_HOME=/data/huggingface  # æ”¹ä¸ºä½ çš„å¤§å®¹é‡ç£ç›˜è·¯å¾„
mkdir -p $HF_HOME

# æ·»åŠ åˆ° ~/.bashrc
echo "export HF_HOME=/data/huggingface" >> ~/.bashrc
```

---

## 7. è¿è¡Œæµ‹è¯•

### 7.1 å¿«é€Ÿç¯å¢ƒæµ‹è¯•

```bash
cd ~/HiDream-E1

# è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
python test_npu_setup.py
```

### 7.2 æµ‹è¯•è®¾å¤‡ç®¡ç†å™¨

```bash
python device_utils.py
```

**é¢„æœŸè¾“å‡ºï¼š**
```
==============================================================
Device Manager Test
==============================================================
ğŸš€ Device Manager initialized
   Device type: npu
   Device: npu:0
   Data type: torch.bfloat16

DeviceManager(device=npu:0, dtype=torch.bfloat16)

ğŸ“ Testing generator...
âœ“ Generator created: <class 'torch.Generator'>

ğŸ“Š Memory statistics:
ğŸ“Š NPU Memory Stats:
   Allocated: 0.00 GB
   Reserved: 0.00 GB
   Peak: 0.00 GB
```

### 7.3 æµ‹è¯• Attention ä¼˜åŒ–å™¨

```bash
python npu_attention_optimizer.py
```

---

## 8. æ‰§è¡Œæ¨ç†

### 8.1 å‡†å¤‡æµ‹è¯•å›¾åƒ

```bash
# æ£€æŸ¥æ˜¯å¦æœ‰æµ‹è¯•å›¾åƒ
ls assets/test_1.png

# å¦‚æœæ²¡æœ‰ï¼Œä¸Šä¼ ä¸€å¼ æµ‹è¯•å›¾ç‰‡
# scp /path/to/test_image.png user@server-ip:~/HiDream-E1/assets/test_1.png
```

### 8.2 è¿è¡Œæ¨ç†ï¼ˆE1.1 ç‰ˆæœ¬ - æ¨èï¼‰

```bash
cd ~/HiDream-E1

# æ–¹æ³• 1: ç›´æ¥è¿è¡Œ Python è„šæœ¬
python inference_e1_1_npu.py
```

**é¦–æ¬¡è¿è¡Œä¼šï¼š**
1. ä¸‹è½½æ¨¡å‹ï¼ˆçº¦ 30-40 GBï¼Œéœ€è¦æ—¶é—´ï¼‰
2. åŠ è½½åˆ° NPUï¼ˆéœ€è¦ 2-3 åˆ†é’Ÿï¼‰
3. æ‰§è¡Œæ¨ç†

**é¢„æœŸè¾“å‡ºï¼š**
```
============================================================
Loading models...
============================================================
Loading Llama tokenizer and text encoder from meta-llama/Llama-3.1-8B-Instruct...
âœ“ Llama model loaded
Loading transformer from HiDream-ai/HiDream-I1-Full...
âœ“ Transformer loaded
...
============================================================
âœ“ Models loaded successfully!
  Device: npu:0
  Data type: torch.bfloat16
ğŸ“Š NPU Memory Stats:
   Allocated: 15.23 GB
   Reserved: 16.00 GB
============================================================
Original size: (1024, 768)
Processed size: (1024, 768)
Instruction: Convert the image into a Ghibli style.
Starting image generation...
...
============================================================
âœ“ Image editing completed successfully!
  Output saved to: results/test_1_npu.jpg
  Metadata saved to: results/test_1_npu.json
  Inference time: 14.32 seconds
============================================================
```

### 8.3 ä½¿ç”¨å¿«é€Ÿå¯åŠ¨è„šæœ¬

```bash
# ç»™è„šæœ¬æ‰§è¡Œæƒé™
chmod +x run_npu.sh

# è¿è¡Œ
./run_npu.sh

# æ ¹æ®æç¤ºé€‰æ‹©ï¼š
# 1) inference_e1_1_npu.py  (E1.1 - æ¨è)
# 2) inference_npu.py       (E1-Full)
# 3) gradio_demo_npu.py     (Web ç•Œé¢)
# 4) test_npu_setup.py      (æµ‹è¯•)
```

### 8.4 è¿è¡Œ Gradio Web ç•Œé¢

```bash
# å¯åŠ¨ Web ç•Œé¢
python gradio_demo_npu.py

# æœåŠ¡ä¼šåœ¨ http://0.0.0.0:7860 å¯åŠ¨
```

**è®¿é—®ç•Œé¢ï¼š**
```bash
# å¦‚æœæœåŠ¡å™¨æœ‰å…¬ç½‘ IP
http://your-server-ip:7860

# å¦‚æœéœ€è¦ SSH ç«¯å£è½¬å‘
# åœ¨æœ¬åœ°æœºå™¨ä¸Šæ‰§è¡Œï¼š
ssh -L 7860:localhost:7860 user@server-ip

# ç„¶åè®¿é—®
http://localhost:7860
```

---

## 9. å¸¸è§é—®é¢˜

### é—®é¢˜ 1: NPU ä¸å¯ç”¨

```bash
# æ£€æŸ¥ npu-smi
npu-smi info

# æ£€æŸ¥é©±åŠ¨
ls /usr/local/Ascend/driver/

# é‡æ–°åŠ è½½ç¯å¢ƒå˜é‡
source ~/.bashrc
source /usr/local/Ascend/ascend-toolkit/latest/set_env.sh
```

### é—®é¢˜ 2: å†…å­˜ä¸è¶³

```bash
# å¯ç”¨ CPU Offload
# ä¿®æ”¹è„šæœ¬ï¼Œåœ¨æ¨¡å‹åŠ è½½åæ·»åŠ ï¼š
pipe.enable_model_cpu_offload()

# æˆ–å‡å°‘æ‰¹å¤„ç†å¤§å°ã€é™ä½åˆ†è¾¨ç‡
```

### é—®é¢˜ 3: ç®—å­ä¸æ”¯æŒ

```bash
# å¯ç”¨å›é€€æ¨¡å¼
export NPU_FALLBACK_MODE=1
python inference_e1_1_npu.py
```

### é—®é¢˜ 4: æ¨¡å‹ä¸‹è½½æ…¢æˆ–å¤±è´¥

```bash
# è®¾ç½®é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com

# æˆ–æ‰‹åŠ¨ä¸‹è½½åæŒ‡å®šæœ¬åœ°è·¯å¾„
# ä¿®æ”¹è„šæœ¬ä¸­çš„æ¨¡å‹è·¯å¾„
```

### é—®é¢˜ 5: bfloat16 ä¸æ”¯æŒ

è„šæœ¬ä¼šè‡ªåŠ¨é™çº§åˆ° float16 æˆ– float32ï¼ŒæŸ¥çœ‹æ—¥å¿—ç¡®è®¤ä½¿ç”¨çš„ç²¾åº¦ã€‚

---

## 10. æ€§èƒ½ä¼˜åŒ–

### 10.1 åˆ‡æ¢åˆ° SDPA Attention

```bash
# ç¼–è¾‘ inference_e1_1_npu.py
vim inference_e1_1_npu.py

# æ‰¾åˆ°ç¬¬ 32 è¡Œï¼Œä¿®æ”¹ï¼š
# os.environ.setdefault("DIFFUSERS_ATTENTION_TYPE", "vanilla")
# æ”¹ä¸ºï¼š
os.environ.setdefault("DIFFUSERS_ATTENTION_TYPE", "sdpa")

# ä¿å­˜åæµ‹è¯•
python inference_e1_1_npu.py
```

### 10.2 å‡å°‘æ¨ç†æ­¥æ•°

```python
# åœ¨ edit_image() è°ƒç”¨æ—¶
steps=20  # ä»é»˜è®¤çš„ 28 æ”¹ä¸º 20
```

### 10.3 è°ƒæ•´åˆ†è¾¨ç‡

```python
# ä¿®æ”¹ resize_image å‡½æ•°
def resize_image(pil_image, image_size=768):  # ä» 1024 é™åˆ° 768
```

### 10.4 å¯ç”¨ç®—å­ä¼˜åŒ–

```bash
# æ·»åŠ ç¯å¢ƒå˜é‡
export ACL_OP_COMPILER_CACHE_MODE=1
export ACL_OP_SELECT_IMPL_MODE=1
```

---

## ğŸ“Š æ€§èƒ½åŸºå‡†å‚è€ƒ

| é…ç½® | æ¨ç†æ—¶é—´ (28 steps) | å†…å­˜å ç”¨ |
|------|---------------------|----------|
| Ascend 910 + bfloat16 + vanilla | ~14-16ç§’ | ~16GB |
| Ascend 910 + bfloat16 + sdpa | ~11-13ç§’ | ~15GB |
| Ascend 910 + float16 | ~13-15ç§’ | ~14GB |

---

## âœ… éƒ¨ç½²æ£€æŸ¥æ¸…å•

- [ ] NPU è®¾å¤‡å¯è§ (`npu-smi info`)
- [ ] CANN å·²å®‰è£…å¹¶é…ç½®ç¯å¢ƒå˜é‡
- [ ] PyTorch å’Œ torch_npu ç‰ˆæœ¬åŒ¹é…
- [ ] `torch.npu.is_available()` è¿”å› True
- [ ] æ‰€æœ‰ä¾èµ–å·²å®‰è£… (`pip list`)
- [ ] HuggingFace å·²ç™»å½•
- [ ] æµ‹è¯•è„šæœ¬é€šè¿‡ (`python test_npu_setup.py`)
- [ ] è®¾å¤‡ç®¡ç†å™¨æ­£å¸¸å·¥ä½œ
- [ ] èƒ½æˆåŠŸè¿è¡Œæ¨ç†

---

## ğŸ“ è·å–å¸®åŠ©

**é‡åˆ°é—®é¢˜æ—¶ï¼š**

1. æŸ¥çœ‹æ—¥å¿—
```bash
python inference_e1_1_npu.py 2>&1 | tee debug.log
```

2. è¿è¡Œè¯Šæ–­
```bash
python test_npu_setup.py > diagnostic.txt 2>&1
```

3. æ£€æŸ¥æ–‡æ¡£
- README_NPU.md
- NPU_MIGRATION_GUIDE.md
- FLASH_ATTENTION_SOLUTIONS.md

4. è”ç³»æ”¯æŒ
- æ˜‡è…¾ç¤¾åŒºï¼šhttps://www.hiascend.com/forum
- GitHub Issuesï¼šhttps://gitee.com/ascend/pytorch

---

ç¥ä½ éƒ¨ç½²é¡ºåˆ©ï¼ğŸš€

# NPU é€‚é…ä¿®æ”¹æ€»ç»“

æœ¬æ–‡æ¡£æ€»ç»“äº†ä¸ºæ”¯æŒåä¸ºæ˜‡è…¾ NPU æ‰€åšçš„æ‰€æœ‰ä»£ç ä¿®æ”¹ã€‚

## ğŸ“¦ æ–°å¢æ–‡ä»¶

### 1. æ ¸å¿ƒå·¥å…·
- **device_utils.py** - è®¾å¤‡ç®¡ç†å·¥å…·ç±»
  - è‡ªåŠ¨æ£€æµ‹ NPU/CUDA/CPU
  - è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ•°æ®ç±»å‹ï¼ˆbfloat16/float16/float32ï¼‰
  - ç»Ÿä¸€çš„å†…å­˜ç®¡ç†å’Œç›‘æ§æ¥å£
  - éšæœºç”Ÿæˆå™¨åˆ›å»º
  - SafeTensors åŠ è½½è¾…åŠ©

### 2. NPU ç‰ˆæœ¬è„šæœ¬
- **inference_e1_1_npu.py** - E1.1 ç‰ˆæœ¬ NPU æ¨ç†è„šæœ¬
  - æ”¯æŒè‡ªåŠ¨è®¾å¤‡æ£€æµ‹
  - é›†æˆè®¾å¤‡ç®¡ç†å™¨
  - ç¦ç”¨ Flash Attention
  - æ·»åŠ æ€§èƒ½ç»Ÿè®¡

- **inference_npu.py** - E1-Full ç‰ˆæœ¬ NPU æ¨ç†è„šæœ¬
  - æ”¯æŒ LoRA æ¨¡å‹
  - æŒ‡ä»¤ä¼˜åŒ–åŠŸèƒ½
  - NPU å…¼å®¹çš„æƒé‡åŠ è½½

- **gradio_demo_npu.py** - NPU ç‰ˆæœ¬äº¤äº’ç•Œé¢
  - Web ç•Œé¢æ”¯æŒ
  - å®æ—¶è®¾å¤‡çŠ¶æ€æ˜¾ç¤º
  - æ‰¹é‡å›¾åƒå¤„ç†

### 3. é…ç½®å’Œæ–‡æ¡£
- **requirements_npu.txt** - NPU ä¸“ç”¨ä¾èµ–åˆ—è¡¨
- **README_NPU.md** - NPU ä½¿ç”¨æŒ‡å—
- **NPU_MIGRATION_GUIDE.md** - è¯¦ç»†çš„è¿ç§»æŠ€æœ¯æ–‡æ¡£
- **test_npu_setup.py** - ç¯å¢ƒæµ‹è¯•è„šæœ¬

## ğŸ”§ æ ¸å¿ƒä¿®æ”¹ç‚¹

### 1. è®¾å¤‡å­—ç¬¦ä¸²æ›¿æ¢

**åŸä»£ç æ¨¡å¼ï¼š**
```python
pipe.to("cuda", torch.bfloat16)
generator = torch.Generator("cuda").manual_seed(seed)
```

**ä¿®æ”¹åï¼š**
```python
from device_utils import DeviceManager
dm = DeviceManager()

pipe.to(dm.device, dm.dtype)
generator = dm.create_generator(seed)
```

**ä¿®æ”¹ä½ç½®ï¼š**
- inference_e1_1.py â†’ inference_e1_1_npu.py
- inference.py â†’ inference_npu.py
- gradio_demo_1_1.py â†’ gradio_demo_npu.py

### 2. å†…å­˜ç›‘æ§é€‚é…

**åŸä»£ç ï¼š**
```python
torch.cuda.memory_summary(device='cuda', abbreviated=True)
```

**ä¿®æ”¹åï¼š**
```python
dm.memory_stats()  # è‡ªåŠ¨é€‚é… NPU/CUDA/CPU
```

### 3. Flash Attention å¤„ç†

**æ·»åŠ çš„ä»£ç ï¼š**
```python
import os
os.environ.setdefault("DIFFUSERS_ATTENTION_TYPE", "vanilla")
```

**åŸå› ï¼š** NPU ä¸æ”¯æŒ CUDA Flash Attentionï¼Œéœ€è¦å›é€€åˆ°æ ‡å‡† attention

### 4. æƒé‡åŠ è½½ä¼˜åŒ–

**åŸä»£ç ï¼š**
```python
lora_ckpt = load_file(lora_ckpt_path, device="cuda")
```

**ä¿®æ”¹åï¼š**
```python
lora_ckpt = dm.load_safetensors(lora_ckpt_path)
# å…ˆåŠ è½½åˆ° CPUï¼Œåç»­è‡ªåŠ¨è½¬ç§»åˆ°ç›®æ ‡è®¾å¤‡
```

### 5. æ•°æ®ç±»å‹è‡ªåŠ¨æ£€æµ‹

**åŸä»£ç ï¼š**
```python
text_encoder = LlamaForCausalLM.from_pretrained(
    ...,
    torch_dtype=torch.bfloat16
)
```

**ä¿®æ”¹åï¼š**
```python
text_encoder = LlamaForCausalLM.from_pretrained(
    ...,
    torch_dtype=DTYPE  # è‡ªåŠ¨æ£€æµ‹çš„æ•°æ®ç±»å‹
)
```

## ğŸ“Š DeviceManager ç±»åŠŸèƒ½

```python
class DeviceManager:
    def __init__(self, preferred_device=None):
        """åˆå§‹åŒ–ï¼Œå¯æŒ‡å®šè®¾å¤‡æˆ–è‡ªåŠ¨æ£€æµ‹"""

    def create_generator(self, seed):
        """åˆ›å»ºè®¾å¤‡å…¼å®¹çš„éšæœºç”Ÿæˆå™¨"""

    def memory_stats(self):
        """æ˜¾ç¤ºå†…å­˜ç»Ÿè®¡ï¼ˆNPU/CUDA/CPU è‡ªé€‚åº”ï¼‰"""

    def empty_cache(self):
        """æ¸…ç©ºè®¾å¤‡ç¼“å­˜"""

    def load_safetensors(self, path):
        """åŠ è½½ safetensors æ–‡ä»¶åˆ°æ­£ç¡®è®¾å¤‡"""

    def set_memory_efficient_mode(self):
        """å¯ç”¨å†…å­˜ä¼˜åŒ–æ¨¡å¼"""
```

## ğŸ¯ ä½¿ç”¨æ–¹å¼å¯¹æ¯”

### åŸå§‹ç‰ˆæœ¬ï¼ˆä»…æ”¯æŒ CUDAï¼‰
```bash
pip install -r requirements.txt
pip install flash-attn --no-build-isolation
python inference_e1_1.py
```

### NPU ç‰ˆæœ¬ï¼ˆæ”¯æŒ NPU/CUDA/CPUï¼‰
```bash
# 1. å®‰è£… CANN å’Œ torch_npuï¼ˆä»… NPU éœ€è¦ï¼‰
# 2. å®‰è£…ä¾èµ–
pip install -r requirements_npu.txt

# 3. æµ‹è¯•ç¯å¢ƒ
python test_npu_setup.py

# 4. è¿è¡Œæ¨ç†ï¼ˆè‡ªåŠ¨æ£€æµ‹è®¾å¤‡ï¼‰
python inference_e1_1_npu.py

# æˆ–æŒ‡å®šè®¾å¤‡
export PREFERRED_DEVICE=npu
python inference_e1_1_npu.py
```

## ğŸ” å…³é”®è®¾è®¡å†³ç­–

### 1. ä¸ºä»€ä¹ˆåˆ›å»ºæ–°æ–‡ä»¶è€Œä¸æ˜¯ä¿®æ”¹åŸæ–‡ä»¶ï¼Ÿ

**ä¼˜ç‚¹ï¼š**
- âœ… ä¿ç•™åŸå§‹ CUDA ç‰ˆæœ¬ä½œä¸ºå‚è€ƒ
- âœ… æ–¹ä¾¿å¯¹æ¯”å’Œè°ƒè¯•
- âœ… ä¸å½±å“åŸæœ‰ç”¨æˆ·
- âœ… å¯ä»¥å¹¶å­˜ä¸¤ä¸ªç‰ˆæœ¬

**ç¼ºç‚¹ï¼š**
- âŒ ä»£ç æœ‰ä¸€å®šé‡å¤
- âŒ éœ€è¦ç»´æŠ¤ä¸¤å¥—ä»£ç 

### 2. ä¸ºä»€ä¹ˆä½¿ç”¨ DeviceManager ç±»ï¼Ÿ

**ä¼˜ç‚¹ï¼š**
- âœ… ç»Ÿä¸€çš„è®¾å¤‡ç®¡ç†æ¥å£
- âœ… è‡ªåŠ¨æ£€æµ‹å’Œå›é€€æœºåˆ¶
- âœ… æ˜“äºæ‰©å±•ï¼ˆæœªæ¥æ”¯æŒæ›´å¤šè®¾å¤‡ï¼‰
- âœ… ä»£ç å¤ç”¨æ€§é«˜

### 3. ä¸ºä»€ä¹ˆç¦ç”¨ Flash Attentionï¼Ÿ

- NPU ä¸æ”¯æŒ CUDA Flash Attention
- æ ‡å‡† attention æ›´å…¼å®¹
- æ€§èƒ½æŸå¤±å¯æ¥å—ï¼ˆ30-50%ï¼‰
- æœªæ¥å¯èƒ½æœ‰ NPU ä¼˜åŒ–ç‰ˆæœ¬

## ğŸ“ å®Œæ•´ä¿®æ”¹æ¸…å•

### A. è®¾å¤‡ç›¸å…³ä¿®æ”¹ï¼ˆ15+ å¤„ï¼‰

| æ–‡ä»¶ | è¡Œå· | åŸä»£ç  | ä¿®æ”¹å |
|-----|------|--------|--------|
| inference_e1_1.py | 107 | `.to("cuda", torch.bfloat16)` | `.to(DEVICE, DTYPE)` |
| inference_e1_1.py | 153 | `torch.Generator("cuda")` | `dm.create_generator(seed)` |
| inference_e1_1.py | 110 | `torch.cuda.memory_summary(...)` | `dm.memory_stats()` |
| inference.py | 38 | `load_file(..., device="cuda")` | `dm.load_safetensors(...)` |
| inference.py | 64 | `.to("cuda", torch.bfloat16)` | `.to(DEVICE, DTYPE)` |
| inference.py | 88 | `torch.Generator("cuda")` | `dm.create_generator(seed)` |
| gradio_demo_1_1.py | 107 | `.to("cuda", torch.bfloat16)` | `.to(DEVICE, DTYPE)` |
| gradio_demo_1_1.py | 125 | `torch.Generator("cuda")` | `dm.create_generator(seed)` |
| ... | ... | ... | ... |

### B. æ–°å¢ä»£ç ï¼ˆæ‰€æœ‰ NPU æ–‡ä»¶ï¼‰

| æ–‡ä»¶ | æ–°å¢å†…å®¹ |
|-----|---------|
| æ‰€æœ‰ NPU è„šæœ¬ | `from device_utils import DeviceManager` |
| æ‰€æœ‰ NPU è„šæœ¬ | `os.environ.setdefault("DIFFUSERS_ATTENTION_TYPE", "vanilla")` |
| æ‰€æœ‰ NPU è„šæœ¬ | è®¾å¤‡ç®¡ç†å™¨åˆå§‹åŒ– |
| æ‰€æœ‰ NPU è„šæœ¬ | æ€§èƒ½ç»Ÿè®¡å’Œæ—¥å¿— |

### C. é…ç½®ä¿®æ”¹

- requirements_npu.txtï¼šç§»é™¤ flash-attn ä¾èµ–
- ç¯å¢ƒå˜é‡æ”¯æŒï¼šPREFERRED_DEVICE

## ğŸ§ª æµ‹è¯•è¦†ç›–

### test_npu_setup.py æµ‹è¯•é¡¹

1. âœ… PyTorch å®‰è£…éªŒè¯
2. âœ… NPU å¯ç”¨æ€§æ£€æµ‹
3. âœ… DeviceManager åŠŸèƒ½æµ‹è¯•
4. âœ… ä¾èµ–åº“å®‰è£…æ£€æŸ¥
5. âœ… ç²¾åº¦æ”¯æŒæµ‹è¯•ï¼ˆfloat32/float16/bfloat16ï¼‰
6. âœ… æ¨¡å‹ä¸‹è½½æµ‹è¯•

## ğŸ“ˆ æ€§èƒ½å½±å“

### é¢„æœŸæ€§èƒ½å˜åŒ–

| ç»„ä»¶ | CUDA | NPUï¼ˆæ— ä¼˜åŒ–ï¼‰ | è¯´æ˜ |
|------|------|---------------|------|
| Attention | Flash Attention | æ ‡å‡† Attention | -30-50% é€Ÿåº¦ |
| Transformer | ä¼˜åŒ–ç®—å­ | å¯èƒ½éœ€è¦å›é€€ | å–å†³äºç®—å­æ”¯æŒ |
| VAE | CUDA ä¼˜åŒ– | NPU åŸç”Ÿ | æ€§èƒ½ç›¸è¿‘ |
| æ€»ä½“æ¨ç† | åŸºå‡† | 70-85% | é¦–æ¬¡è¿ç§»é¢„æœŸ |

### ä¼˜åŒ–æ½œåŠ›

- ğŸ”§ ä½¿ç”¨æ˜‡è…¾ä¼˜åŒ–çš„ Attentionï¼ˆå¦‚æœå¯ç”¨ï¼‰
- ğŸ”§ å›¾ç¼–è¯‘å’Œç®—å­èåˆ
- ğŸ”§ æ··åˆç²¾åº¦ä¼˜åŒ–
- ğŸ”§ æ‰¹å¤„ç†ä¼˜åŒ–

## ğŸš€ åç»­æ”¹è¿›æ–¹å‘

### çŸ­æœŸï¼ˆ1-2å‘¨ï¼‰
- [ ] å¯»æ‰¾ NPU ä¼˜åŒ–çš„ Attention å®ç°
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•å’Œå¯¹æ¯”
- [ ] æ·»åŠ æ›´å¤šæµ‹è¯•ç”¨ä¾‹
- [ ] ä¼˜åŒ–å†…å­˜ä½¿ç”¨

### ä¸­æœŸï¼ˆ1ä¸ªæœˆï¼‰
- [ ] æ¢ç´¢å›¾ç¼–è¯‘ä¼˜åŒ–
- [ ] ç®—å­çº§æ€§èƒ½åˆ†æ
- [ ] æ”¯æŒæ›´å¤šæ˜‡è…¾è®¾å¤‡å‹å·
- [ ] æ–‡æ¡£å®Œå–„

### é•¿æœŸï¼ˆ2-3ä¸ªæœˆï¼‰
- [ ] åŸåœ°ä¿®æ”¹åŸå§‹æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
- [ ] æ€§èƒ½ä¼˜åŒ–åˆ°æ¥è¿‘ CUDA æ°´å¹³
- [ ] æ”¯æŒåˆ†å¸ƒå¼æ¨ç†
- [ ] ç¤¾åŒºåé¦ˆæ•´åˆ

## ğŸ“š å‚è€ƒèµ„æº

- **åä¸ºæ˜‡è…¾æ–‡æ¡£**: https://www.hiascend.com/document
- **torch_npu ä»“åº“**: https://gitee.com/ascend/pytorch
- **CANN å¼€å‘æŒ‡å—**: https://www.hiascend.com/document/detail/zh/CANNCommunityEdition
- **æ˜‡è…¾ç¤¾åŒºè®ºå›**: https://www.hiascend.com/forum

## âœ… éªŒæ”¶æ ‡å‡†

ä»£ç ä¿®æ”¹æˆåŠŸçš„æ ‡å‡†ï¼š

1. âœ… åœ¨ NPU ç¯å¢ƒä¸‹èƒ½æˆåŠŸåŠ è½½æ¨¡å‹
2. âœ… èƒ½å®Œæˆç«¯åˆ°ç«¯æ¨ç†
3. âœ… è¾“å‡ºå›¾åƒè´¨é‡ä¸ CUDA ç‰ˆæœ¬ç›¸å½“
4. âœ… æ— ä¸¥é‡å†…å­˜æ³„æ¼
5. âœ… é”™è¯¯å¤„ç†å¥å£®ï¼ˆç®—å­ä¸æ”¯æŒæ—¶èƒ½å›é€€ï¼‰
6. âœ… æ—¥å¿—æ¸…æ™°ï¼Œä¾¿äºè°ƒè¯•
7. âœ… æ–‡æ¡£å®Œæ•´ï¼Œæ˜“äºä½¿ç”¨

---

## è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·ï¼š
1. æŸ¥çœ‹ README_NPU.md ä½¿ç”¨æŒ‡å—
2. æŸ¥çœ‹ NPU_MIGRATION_GUIDE.md æŠ€æœ¯æ–‡æ¡£
3. è¿è¡Œ test_npu_setup.py è¯Šæ–­ç¯å¢ƒ
4. æäº¤ Issue æˆ–è”ç³»å¼€å‘å›¢é˜Ÿ

æœ€åæ›´æ–°ï¼š2025-10-16

# ä½¿ç”¨æœ¬åœ°æ¨¡å‹è¿è¡Œ HiDream-E1

æœ¬æŒ‡å—è¯´æ˜å¦‚ä½•å°†æ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°å¹¶ä¿®æ”¹ä»£ç ä½¿ç”¨æœ¬åœ°è·¯å¾„ã€‚

## ğŸ“¦ éœ€è¦ä¸‹è½½çš„æ¨¡å‹

æ€»å…±éœ€è¦ 3 ä¸ªæ¨¡å‹ï¼Œæ€»å¤§å°çº¦ **40-50 GB**ï¼š

1. **meta-llama/Llama-3.1-8B-Instruct** (~16 GB)
2. **HiDream-ai/HiDream-I1-Full** (~15 GB)
3. **HiDream-ai/HiDream-E1-1** (~15 GB)

---

## ğŸ“¥ ç¬¬ä¸€æ­¥ï¼šä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°

### æ–¹æ³• A: ä½¿ç”¨ huggingface-cliï¼ˆæ¨èï¼‰

åœ¨**æœ‰ç½‘ç»œçš„æœºå™¨**ä¸Šï¼ˆå¦‚ä½ çš„ Macï¼‰ï¼š

```bash
# 1. å®‰è£… huggingface-cli
pip install -U huggingface_hub

# 2. ç™»å½•ï¼ˆéœ€è¦å…ˆåœ¨ç½‘é¡µåŒæ„ Llama åè®®ï¼‰
huggingface-cli login

# 3. åˆ›å»ºæ¨¡å‹å­˜å‚¨ç›®å½•
mkdir -p ~/Downloads/hidream_models

# 4. ä¸‹è½½ Llama-3.1-8B
huggingface-cli download meta-llama/Llama-3.1-8B-Instruct \
    --local-dir ~/Downloads/hidream_models/Llama-3.1-8B-Instruct

# 5. ä¸‹è½½ HiDream-I1-Full
huggingface-cli download HiDream-ai/HiDream-I1-Full \
    --local-dir ~/Downloads/hidream_models/HiDream-I1-Full

# 6. ä¸‹è½½ HiDream-E1.1
huggingface-cli download HiDream-ai/HiDream-E1-1 \
    --local-dir ~/Downloads/hidream_models/HiDream-E1-1
```

### æ–¹æ³• B: ä½¿ç”¨ Python è„šæœ¬

åˆ›å»º `download_models.py`ï¼š

```python
from huggingface_hub import snapshot_download
import os

# æ¨¡å‹ä¿å­˜ç›®å½•
save_dir = os.path.expanduser("~/Downloads/hidream_models")
os.makedirs(save_dir, exist_ok=True)

models = [
    "meta-llama/Llama-3.1-8B-Instruct",
    "HiDream-ai/HiDream-I1-Full",
    "HiDream-ai/HiDream-E1-1",
]

for model_id in models:
    print(f"\n{'='*60}")
    print(f"ä¸‹è½½: {model_id}")
    print(f"{'='*60}")

    model_name = model_id.split("/")[-1]
    local_path = os.path.join(save_dir, model_name)

    snapshot_download(
        repo_id=model_id,
        local_dir=local_path,
        local_dir_use_symlinks=False,
        resume_download=True,  # æ”¯æŒæ–­ç‚¹ç»­ä¼ 
    )

    print(f"âœ“ {model_id} å·²ä¸‹è½½åˆ°: {local_path}")

print(f"\nâœ“ æ‰€æœ‰æ¨¡å‹å·²ä¸‹è½½åˆ°: {save_dir}")
```

è¿è¡Œï¼š
```bash
python download_models.py
```

---

## ğŸ“¤ ç¬¬äºŒæ­¥ï¼šä¸Šä¼ åˆ°æœåŠ¡å™¨

```bash
# å‹ç¼©æ¨¡å‹ï¼ˆå¯é€‰ï¼ŒèŠ‚çœä¼ è¾“æ—¶é—´ï¼‰
cd ~/Downloads
tar -czf hidream_models.tar.gz hidream_models/

# ä¸Šä¼ åˆ°æœåŠ¡å™¨
scp hidream_models.tar.gz user@server:/home/ma-user/work/jiangtianyi/

# SSH ç™»å½•æœåŠ¡å™¨è§£å‹
ssh user@server
cd /home/ma-user/work/jiangtianyi/
tar -xzf hidream_models.tar.gz
```

æˆ–è€…ä¸å‹ç¼©ç›´æ¥ä¼ ï¼ˆæ—¶é—´è¾ƒé•¿ï¼‰ï¼š
```bash
rsync -avz --progress \
    ~/Downloads/hidream_models/ \
    user@server:/home/ma-user/work/jiangtianyi/hidream_models/
```

---

## ğŸ”§ ç¬¬ä¸‰æ­¥ï¼šä¿®æ”¹ä»£ç 

### æ–¹æ³• Aï¼šåˆ›å»ºé…ç½®æ–‡ä»¶ï¼ˆæ¨èï¼‰

åœ¨æœåŠ¡å™¨ä¸Šåˆ›å»º `local_config.py`ï¼š

```python
"""
æœ¬åœ°æ¨¡å‹è·¯å¾„é…ç½®
"""
import os

# ========== é…ç½®ä½ çš„æ¨¡å‹è·¯å¾„ ==========
MODEL_BASE_DIR = "/home/ma-user/work/jiangtianyi/hidream_models"

# æ¨¡å‹è·¯å¾„
LLAMA_PATH = os.path.join(MODEL_BASE_DIR, "Llama-3.1-8B-Instruct")
HIDREAM_I1_PATH = os.path.join(MODEL_BASE_DIR, "HiDream-I1-Full")
HIDREAM_E1_PATH = os.path.join(MODEL_BASE_DIR, "HiDream-E1-1")

# éªŒè¯è·¯å¾„æ˜¯å¦å­˜åœ¨
def verify_models():
    """æ£€æŸ¥æ‰€æœ‰æ¨¡å‹æ˜¯å¦å­˜åœ¨"""
    models = {
        "Llama-3.1-8B": LLAMA_PATH,
        "HiDream-I1-Full": HIDREAM_I1_PATH,
        "HiDream-E1-1": HIDREAM_E1_PATH,
    }

    all_exist = True
    for name, path in models.items():
        if os.path.exists(path):
            print(f"âœ“ {name:20s} -> {path}")
        else:
            print(f"âœ— {name:20s} -> {path} [ä¸å­˜åœ¨]")
            all_exist = False

    return all_exist

if __name__ == "__main__":
    print("æ£€æŸ¥æœ¬åœ°æ¨¡å‹:")
    print("="*60)
    if verify_models():
        print("="*60)
        print("âœ“ æ‰€æœ‰æ¨¡å‹è·¯å¾„æ­£ç¡®")
    else:
        print("="*60)
        print("âœ— éƒ¨åˆ†æ¨¡å‹æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥è·¯å¾„")
```

éªŒè¯é…ç½®ï¼š
```bash
python local_config.py
```

### æ–¹æ³• Bï¼šç›´æ¥ä¿®æ”¹æ¨ç†è„šæœ¬

ç¼–è¾‘ `inference_e1_1_npu.py`ï¼Œæ‰¾åˆ°ç¬¬ 20-23 è¡Œï¼š

```python
# åŸä»£ç ï¼š
# LLAMA_PATH = "meta-llama/Llama-3.1-8B-Instruct"
# HIDREAM_I1_PATH = "HiDream-ai/HiDream-I1-Full"
# HIDREAM_E1_PATH = "HiDream-ai/HiDream-E1-1"

# æ”¹ä¸ºï¼š
LLAMA_PATH = "/home/ma-user/work/jiangtianyi/hidream_models/Llama-3.1-8B-Instruct"
HIDREAM_I1_PATH = "/home/ma-user/work/jiangtianyi/hidream_models/HiDream-I1-Full"
HIDREAM_E1_PATH = "/home/ma-user/work/jiangtianyi/hidream_models/HiDream-E1-1"
```

---

## âœ… ç¬¬å››æ­¥ï¼šåˆ›å»ºä½¿ç”¨æœ¬åœ°æ¨¡å‹çš„è„šæœ¬

æˆ‘å¸®ä½ åˆ›å»ºä¸€ä¸ªæ–°ç‰ˆæœ¬ `inference_e1_1_local.py`ï¼š

```python
import torch
from transformers import PreTrainedTokenizerFast, LlamaForCausalLM
from pipeline_hidream_image_editing import HiDreamImageEditingPipeline
from PIL import Image
from diffusers import HiDreamImageTransformer2DModel
import json
import os
from collections import defaultdict
from safetensors.torch import safe_open
import math
import logging

# ============ NPU æ”¯æŒ ============
from device_utils import DeviceManager
# ==================================

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)

# ============ ä½¿ç”¨æœ¬åœ°æ¨¡å‹é…ç½® ============
try:
    from local_config import LLAMA_PATH, HIDREAM_I1_PATH, HIDREAM_E1_PATH, verify_models
    logging.info("ä½¿ç”¨æœ¬åœ°æ¨¡å‹é…ç½®")
    if not verify_models():
        raise FileNotFoundError("æœ¬åœ°æ¨¡å‹è·¯å¾„æ£€æŸ¥å¤±è´¥")
except ImportError:
    # å¦‚æœæ²¡æœ‰ local_config.pyï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„
    logging.warning("æœªæ‰¾åˆ° local_config.pyï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹è·¯å¾„")
    LLAMA_PATH = "meta-llama/Llama-3.1-8B-Instruct"
    HIDREAM_I1_PATH = "HiDream-ai/HiDream-I1-Full"
    HIDREAM_E1_PATH = "HiDream-ai/HiDream-E1-1"
# ==========================================

# ============ åˆå§‹åŒ–è®¾å¤‡ç®¡ç†å™¨ ============
preferred_device = os.environ.get('PREFERRED_DEVICE', None)
device_manager = DeviceManager(preferred_device=preferred_device)
DEVICE = device_manager.device
DTYPE = device_manager.dtype
# =========================================

# Flash Attention å¤„ç†
os.environ.setdefault("DIFFUSERS_ATTENTION_TYPE", "vanilla")
logging.info(f"Attention type: {os.environ.get('DIFFUSERS_ATTENTION_TYPE', 'default')}")

# ... (å…¶ä½™ä»£ç ä¸ inference_e1_1_npu.py å®Œå…¨ç›¸åŒ)
```

---

## ğŸš€ ç¬¬äº”æ­¥ï¼šè¿è¡Œ

```bash
cd /home/ma-user/work/jiangtianyi/HiDream-E1

# 1. éªŒè¯æ¨¡å‹è·¯å¾„
python local_config.py

# 2. è¿è¡Œæ¨ç†ï¼ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼‰
python inference_e1_1_local.py
```

---

## ğŸ“ æœ€ç»ˆç›®å½•ç»“æ„

```
/home/ma-user/work/jiangtianyi/
â”œâ”€â”€ HiDream-E1/                    # é¡¹ç›®ä»£ç 
â”‚   â”œâ”€â”€ local_config.py           # æœ¬åœ°æ¨¡å‹é…ç½® (æ–°å»º)
â”‚   â”œâ”€â”€ inference_e1_1_local.py   # ä½¿ç”¨æœ¬åœ°æ¨¡å‹çš„æ¨ç†è„šæœ¬ (æ–°å»º)
â”‚   â”œâ”€â”€ device_utils.py
â”‚   â”œâ”€â”€ test_npu_setup.py
â”‚   â””â”€â”€ ...
â””â”€â”€ hidream_models/                # æ¨¡å‹æ–‡ä»¶
    â”œâ”€â”€ Llama-3.1-8B-Instruct/    # ~16 GB
    â”‚   â”œâ”€â”€ config.json
    â”‚   â”œâ”€â”€ tokenizer.json
    â”‚   â”œâ”€â”€ model-*.safetensors
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ HiDream-I1-Full/           # ~15 GB
    â”‚   â”œâ”€â”€ transformer/
    â”‚   â”œâ”€â”€ vae/
    â”‚   â””â”€â”€ ...
    â””â”€â”€ HiDream-E1-1/              # ~15 GB
        â”œâ”€â”€ transformer/
        â””â”€â”€ ...
```

---

## ğŸ” éªŒè¯æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§

```bash
# æ£€æŸ¥æ¯ä¸ªæ¨¡å‹ç›®å½•
ls -lh /home/ma-user/work/jiangtianyi/hidream_models/Llama-3.1-8B-Instruct/
ls -lh /home/ma-user/work/jiangtianyi/hidream_models/HiDream-I1-Full/
ls -lh /home/ma-user/work/jiangtianyi/hidream_models/HiDream-E1-1/

# åº”è¯¥çœ‹åˆ° config.json, model æ–‡ä»¶ç­‰
```

---

## âš ï¸ å¸¸è§é—®é¢˜

### Q1: æ¨¡å‹ä¸‹è½½ä¸­æ–­æ€ä¹ˆåŠï¼Ÿ

ä½¿ç”¨ `resume_download=True` å‚æ•°ï¼Œå¯ä»¥æ–­ç‚¹ç»­ä¼ ï¼š

```python
snapshot_download(
    repo_id=model_id,
    local_dir=local_path,
    resume_download=True,  # æ–­ç‚¹ç»­ä¼ 
)
```

### Q2: ç©ºé—´ä¸å¤Ÿæ€ä¹ˆåŠï¼Ÿ

```bash
# æ£€æŸ¥ç£ç›˜ç©ºé—´
df -h

# å¦‚æœ home ç›®å½•ç©ºé—´ä¸è¶³ï¼Œä½¿ç”¨å…¶ä»–ç›®å½•
# ä¿®æ”¹ local_config.py ä¸­çš„ MODEL_BASE_DIR
MODEL_BASE_DIR = "/data/models"  # æ”¹ä¸ºç©ºé—´å……è¶³çš„ç›®å½•
```

### Q3: ä¸Šä¼ å¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ

```bash
# ä½¿ç”¨å‹ç¼©å¯ä»¥å‡å°‘ä½“ç§¯çº¦ 20-30%
tar -czf hidream_models.tar.gz hidream_models/

# æˆ–è€…åˆ†æ‰¹ä¸Šä¼ 
rsync -avz --progress hidream_models/Llama-3.1-8B-Instruct/ server:~/models/llama/
rsync -avz --progress hidream_models/HiDream-I1-Full/ server:~/models/hidream-i1/
rsync -avz --progress hidream_models/HiDream-E1-1/ server:~/models/hidream-e1/
```

---

## âœ… å®Œæ•´æµç¨‹æ£€æŸ¥æ¸…å•

- [ ] åœ¨æœ¬åœ°ä¸‹è½½æ‰€æœ‰æ¨¡å‹ï¼ˆçº¦ 40-50 GBï¼‰
- [ ] ä¸Šä¼ åˆ°æœåŠ¡å™¨
- [ ] åˆ›å»º `local_config.py` é…ç½®æ–‡ä»¶
- [ ] éªŒè¯æ¨¡å‹è·¯å¾„ï¼š`python local_config.py`
- [ ] åˆ›å»ºæˆ–ä¿®æ”¹æ¨ç†è„šæœ¬ä½¿ç”¨æœ¬åœ°è·¯å¾„
- [ ] è¿è¡Œæµ‹è¯•ï¼š`python inference_e1_1_local.py`

å®Œæˆåï¼Œä½ å°±ä¸å†ä¾èµ–ç½‘ç»œä¸‹è½½äº†ï¼
